var documenterSearchIndex = {"docs":
[{"location":"backend/#How-to-switch-backend-in-ADNLPModels","page":"Backend","title":"How to switch backend in ADNLPModels","text":"","category":"section"},{"location":"backend/","page":"Backend","title":"Backend","text":"ADNLPModels allows the use of different backends to compute the derivatives required within NLPModel API. It uses ForwardDiff.jl, ReverseDiff.jl, and more via optional depencies.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"The backend information is in a structure ADNLPModels.ADModelBackend in the attribute adbackend of a ADNLPModel, it can also be accessed with get_adbackend.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"The functions used internally to define the NLPModel API and the possible backends are defined in the following table:","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"Functions FowardDiff backends ReverseDiff backends Zygote backends Enzyme backend Sparse backend\ngradient and gradient! ForwardDiffADGradient/GenericForwardDiffADGradient ReverseDiffADGradient/GenericReverseDiffADGradient ZygoteADGradient EnzymeADGradient –\njacobian ForwardDiffADJacobian ReverseDiffADJacobian ZygoteADJacobian – SparseADJacobian\nhessian ForwardDiffADHessian ReverseDiffADHessian ZygoteADHessian – SparseADHessian/SparseReverseADHessian\nJprod ForwardDiffADJprod/GenericForwardDiffADJprod ReverseDiffADJprod/GenericReverseDiffADJprod ZygoteADJprod – –\nJtprod ForwardDiffADJtprod/GenericForwardDiffADJtprod ReverseDiffADJtprod/GenericReverseDiffADJtprod ZygoteADJtprod – –\nHvprod ForwardDiffADHvprod/GenericForwardDiffADHvprod ReverseDiffADHvprod/GenericReverseDiffADHvprod – – –\ndirectional_second_derivative ForwardDiffADGHjvprod – – – –","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"The functions hess_structure!, hess_coord!, jac_structure! and jac_coord! defined in ad.jl are generic to all the backends for now.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"using ADNLPModels\nf(x) = sum(x)\nx0 = ones(2)\nADNLPModel(f, x0, show_time = true)","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"The keyword show_time is set to true to display the time needed to instantiate each backend. For unconstrained problem, there is no need to compute derivatives of constraints so an EmptyADbackend is used for Jacobian computations.","category":"page"},{"location":"backend/#Examples","page":"Backend","title":"Examples","text":"","category":"section"},{"location":"backend/","page":"Backend","title":"Backend","text":"We now present a serie of practical examples. For simplicity, we focus here on unconstrained optimization problem. All these examples can be generalized to problems with bounds, constraints or nonlinear least-squares.","category":"page"},{"location":"backend/#Use-another-backend","page":"Backend","title":"Use another backend","text":"","category":"section"},{"location":"backend/","page":"Backend","title":"Backend","text":"As shown in Tutorial, it is very straightforward to instantiate an ADNLPModel using an objective function and an initial guess.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"using ADNLPModels, NLPModels\nf(x) = sum(x)\nx0 = ones(3)\nnlp = ADNLPModel(f, x0)\ngrad(nlp, nlp.meta.x0) # returns the gradient at x0","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"Thanks to the backends inside ADNLPModels.jl, it is easy to change the backend for one (or more) function using the kwargs presented in the table above.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"nlp = ADNLPModel(f, x0, gradient_backend = ADNLPModels.ReverseDiffADGradient)\ngrad(nlp, nlp.meta.x0)  # returns the gradient at x0 using `ReverseDiff`","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"It is also possible to try some new implementation for each function. First, we define a new ADBackend structure.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"struct NewADGradient <: ADNLPModels.ADBackend end\nfunction NewADGradient(\n  nvar::Integer,\n  f,\n  ncon::Integer = 0,\n  c::Function = (args...) -> [];\n  kwargs...,\n)\n  return NewADGradient()\nend","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"Then, we implement the desired functions following the table above.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"ADNLPModels.gradient(adbackend::NewADGradient, f, x) = rand(Float64, size(x))\nfunction ADNLPModels.gradient!(adbackend::NewADGradient, g, f, x)\n  g .= rand(Float64, size(x))\n  return g\nend","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"Finally, we use the homemade backend to compute the gradient.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"nlp = ADNLPModel(sum, ones(3), gradient_backend = NewADGradient)\ngrad(nlp, nlp.meta.x0)  # returns the gradient at x0 using `NewADGradient`","category":"page"},{"location":"backend/#Change-backend","page":"Backend","title":"Change backend","text":"","category":"section"},{"location":"backend/","page":"Backend","title":"Backend","text":"Once an instance of an ADNLPModel has been created, it is possible to change the backends without re-instantiating the model.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"using ADNLPModels, NLPModels\nf(x) = 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = 3 * ones(2)\nnlp = ADNLPModel(f, x0)\nget_adbackend(nlp) # returns the `ADModelBackend` structure that regroup all the various backends.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"There are currently two ways to modify instantiated backends. The first one is to instantiate a new ADModelBackend and use set_adbackend! to modify nlp.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"adback = ADNLPModels.ADModelBackend(nlp.meta.nvar, nlp.f, gradient_backend = ADNLPModels.ForwardDiffADGradient)\nset_adbackend!(nlp, adback)\nget_adbackend(nlp)","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"The alternative is to use set_adbackend! and pass the new backends via kwargs. In the second approach, it is possible to pass either the type of the desired backend or an instance as shown below.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"set_adbackend!(\n  nlp,\n  gradient_backend = ADNLPModels.ForwardDiffADGradient,\n  jtprod_backend = ADNLPModels.GenericForwardDiffADJtprod(),\n)\nget_adbackend(nlp)","category":"page"},{"location":"backend/#Support-multiple-precision-without-having-to-recreate-the-model","page":"Backend","title":"Support multiple precision without having to recreate the model","text":"","category":"section"},{"location":"backend/","page":"Backend","title":"Backend","text":"One of the strength of ADNLPModels.jl is the type flexibility. Let's assume, we first instantiate an ADNLPModel with a Float64 initial guess.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"using ADNLPModels, NLPModels\nf(x) = 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = 3 * ones(2) # Float64 initial guess\nnlp = ADNLPModel(f, x0)","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"Then, the gradient will return a vector of Float64.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"x64 = rand(2)\ngrad(nlp, x64)","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"It is now possible to move to a different type, for instance Float32, while keeping the instance nlp.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"x0_32 = ones(Float32, 2)\nset_adbackend!(nlp, gradient_backend = ADNLPModels.ForwardDiffADGradient, x0 = x0_32)\nx32 = rand(Float32, 2)\ngrad(nlp, x32)","category":"page"},{"location":"performance/#Performance-tips","page":"Performance tips","title":"Performance tips","text":"","category":"section"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"The package ADNLPModels.jl is designed to easily model optimization problems andto allow an efficient access to the NLPModel API. In this tutorial, we will see some tips to ensure the maximum performance of the model.","category":"page"},{"location":"performance/#Use-in-place-constructor","page":"Performance tips","title":"Use in-place constructor","text":"","category":"section"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"When dealing with a constrained optimization problem, it is recommended to use in-place constraint functions.","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"using ADNLPModels, NLPModels\nf(x) = sum(x)\nx0 = ones(2)\nlcon = ucon = ones(1)\nc_out(x) = [x[1]]\nnlp_out = ADNLPModel(f, x0, c_out, lcon, ucon)\n\nc_in(cx, x) = begin\n  cx[1] = x[1]\n  return cx\nend\nnlp_in = ADNLPModel!(f, x0, c_in, lcon, ucon)","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"using BenchmarkTools\ncx = rand(1)\nx = 18 * ones(2)\n@btime cons!(nlp_out, x, cx)","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"@btime cons!(nlp_in, x, cx)","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"The difference between the two increases with the dimension.","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"Note that the same applies to nonlinear least squares problems.","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"F(x) = [\n    x[1];\n    x[1] + x[2]^2;\n    sin(x[2]);\n    exp(x[1] + 0.5)\n]\nx0 = ones(2)\nnequ = 4\nnls_out = ADNLSModel(F, x0, nequ)\n\nF!(Fx, x) = begin\n  Fx[1] = x[1]\n  Fx[2] = x[1] + x[2]^2\n  Fx[3] = sin(x[2])\n  Fx[4] = exp(x[1] + 0.5)\n  return Fx\nend\nnls_in = ADNLSModel!(F!, x0, nequ)","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"Fx = rand(4)\n@btime residual!(nls_out, x, Fx)","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"@btime residual!(nls_in, x, Fx)","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"This phenomenon also extends to related backends.","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"Fx = rand(4)\nv = ones(2)\n@btime jprod_residual!(nls_out, x, v, Fx)","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"@btime jprod_residual!(nls_in, x, v, Fx)","category":"page"},{"location":"performance/#Use-only-the-needed-operations","page":"Performance tips","title":"Use only the needed operations","text":"","category":"section"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"It is tempting to define the most generic and efficient ADNLPModel from the start.","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"using ADNLPModels, NLPModels\nf(x) = (x[1] - x[2])^2\nx0 = ones(2)\nlcon = ucon = ones(1)\nc_in(cx, x) = begin\n  cx[1] = x[1]\n  return cx\nend\nnlp = ADNLPModel!(f, x0, c_in, lcon, ucon, show_time = true)","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"However, depending on the size of the problem this might time consuming as initializing each backend takes time. Besides, some solvers may not require all the API to solve the problem. For instance, Percival.jl is matrix-free solver in the sense that it only uses jprod, jtprod and hprod.","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"using Percival\nstats = percival(nlp)","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"nlp.counters","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"Therefore, it is more efficient to avoid preparing Jacobian and Hessian backends in this case.","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"nlp = ADNLPModel!(f, x0, c_in, lcon, ucon, jacobian_backend = ADNLPModels.EmptyADbackend, hessian_backend = ADNLPModels.EmptyADbackend, show_time = true)","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"or, equivalently, using the matrix_free keyword argument","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"nlp = ADNLPModel!(f, x0, c_in, lcon, ucon, show_time = true, matrix_free = true)","category":"page"},{"location":"performance/#Benchmarks","page":"Performance tips","title":"Benchmarks","text":"","category":"section"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"This package implements several backends for each method and it is possible to design your own backend as well.  Then, one way to choose the most efficient one is to run benchmarks.","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"using ADNLPModels, NLPModels, OptimizationProblems","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"The package OptimizationProblems.jl provides a collection of optimization problems in JuMP and ADNLPModels syntax.","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"meta = OptimizationProblems.meta;","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"We select the problems that are scalable, so that there size can be modified. By default, the size is close to 100.","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"scalable_problems = meta[(meta.variable_nvar .== true) .& (meta.ncon .> 0), :name]","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"using NLPModelsJuMP\nlist_backends = Dict(\n  :forward => ADNLPModels.ForwardDiffADGradient,\n  :reverse => ADNLPModels.ReverseDiffADGradient,\n)","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"using DataFrames\nnprob = length(scalable_problems)\nstats = Dict{Symbol, DataFrame}()\nfor back in union(keys(list_backends), [:jump])\n  stats[back] = DataFrame(\"name\" => scalable_problems,\n                 \"time\" => zeros(nprob),\n                 \"allocs\" => zeros(Int, nprob))\nend","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"using BenchmarkTools\nnscal = 1000\nfor name in scalable_problems\n  n = eval(Meta.parse(\"OptimizationProblems.get_\" * name * \"_nvar(n = $(nscal))\"))\n  m = eval(Meta.parse(\"OptimizationProblems.get_\" * name * \"_ncon(n = $(nscal))\"))\n  @info \" $(name) with $n vars and $m cons\"\n  global x = ones(n)\n  global g = zeros(n)\n  global pb = Meta.parse(name)\n  global nlp = MathOptNLPModel(OptimizationProblems.PureJuMP.eval(pb)(n = nscal))\n  b = @benchmark grad!(nlp, x, g)\n  stats[:jump][stats[:jump].name .== name, :time] = [median(b.times)]\n  stats[:jump][stats[:jump].name .== name, :allocs] = [median(b.allocs)]\n  for back in keys(list_backends)\n    nlp = OptimizationProblems.ADNLPProblems.eval(pb)(n = nscal, gradient_backend = list_backends[back], matrix_free = true)\n    b = @benchmark grad!(nlp, x, g)\n    stats[back][stats[back].name .== name, :time] = [median(b.times)]\n    stats[back][stats[back].name .== name, :allocs] = [median(b.allocs)]\n  end\nend","category":"page"},{"location":"performance/","page":"Performance tips","title":"Performance tips","text":"using Plots, SolverBenchmark\ncostnames = [\"median time (in ns)\", \"median allocs\"]\ncosts = [\n  df -> df.time,\n  df -> df.allocs,\n]\n\ngr()\n\nprofile_solvers(stats, costs, costnames)","category":"page"},{"location":"predefined/#Default-backend-and-performance-in-ADNLPModels","page":"Default backends","title":"Default backend and performance in ADNLPModels","text":"","category":"section"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"As illustrated in the tutorial on backends, ADNLPModels.jl use different backend for each method from the NLPModel API that are implemented. By default, it uses the following:","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"using ADNLPModels, NLPModels\n\nf(x) = 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nT = Float64\nx0 = T[-1.2; 1.0]\nlvar, uvar = zeros(T, 2), ones(T, 2) # must be of same type than `x0`\nlcon, ucon = -T[0.5], T[0.5]\nc!(cx, x) = begin\n  cx[1] = x[1] + x[2]\n  return cx\nend\nnlp = ADNLPModel!(f, x0, lvar, uvar, c!, lcon, ucon)\nget_adbackend(nlp)","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"Note that ForwardDiff.jl is mainly used as it is efficient and stable.","category":"page"},{"location":"predefined/#Predefined-backends","page":"Default backends","title":"Predefined backends","text":"","category":"section"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"Another way to know the default backends used is to check the constant ADNLPModels.default_backend.","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"ADNLPModels.default_backend","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"More generally, the package anticipates more uses","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"ADNLPModels.predefined_backend","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"The backend :optimized will mainly focus on the most efficient approaches, for instance using ReverseDiff to compute the gradient instead of ForwardDiff.","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"ADNLPModels.predefined_backend[:optimized]","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"The backend :generic focuses on backend that make no assumptions on the element type, see Creating an ADNLPModels backend that supports multiple precisions.","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"It is possible to use these pre-defined backends using the keyword argument backend when instantiating the model.","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"nlp = ADNLPModel!(f, x0, lvar, uvar, c!, lcon, ucon, backend = :optimized)\nget_adbackend(nlp)","category":"page"},{"location":"mixed/#Build-a-hybrid-NLPModel","page":"Build a hybrid NLPModel","title":"Build a hybrid NLPModel","text":"","category":"section"},{"location":"mixed/","page":"Build a hybrid NLPModel","title":"Build a hybrid NLPModel","text":"The package ADNLPModels.jl implements the NLPModel API using automatic differentiation (AD) backends. It is also possible to build hybrid models that use AD to complete the implementation of a given NLPModel.","category":"page"},{"location":"mixed/","page":"Build a hybrid NLPModel","title":"Build a hybrid NLPModel","text":"In the following example, we use ManualNLPModels.jl to build an NLPModel with the gradient and the Jacobian functions implemented.","category":"page"},{"location":"mixed/","page":"Build a hybrid NLPModel","title":"Build a hybrid NLPModel","text":"using ManualNLPModels\nf(x) = (x[1] - 1)^2 + 4 * (x[2] - x[1]^2)^2\ng!(gx, x) = begin\n  y1, y2 = x[1] - 1, x[2] - x[1]^2\n  gx[1] = 2 * y1 - 16 * x[1] * y2\n  gx[2] = 8 * y2\n  return gx\nend\n\nc!(cx, x) = begin\n  cx[1] = x[1] + x[2]\n  return cx\nend\nj!(vals, x) = begin\n  vals[1] = 1.0\n  vals[2] = 1.0\n  return vals\nend\n\nx0 = [-1.2; 1.0]\nmodel = NLPModel(\n  x0,\n  f,\n  grad = g!,\n  cons = (c!, [0.0], [0.0]),\n  jac_coord = ([1; 1], [1; 2], j!),\n)","category":"page"},{"location":"mixed/","page":"Build a hybrid NLPModel","title":"Build a hybrid NLPModel","text":"However, methods involving the Hessian or Jacobian-vector products are not implemented.","category":"page"},{"location":"mixed/","page":"Build a hybrid NLPModel","title":"Build a hybrid NLPModel","text":"using NLPModels\nv = ones(2)\ntry\n  jprod(model, x0, v)\ncatch e\n  println(\"$e\")\nend","category":"page"},{"location":"mixed/","page":"Build a hybrid NLPModel","title":"Build a hybrid NLPModel","text":"This is where building hybrid models with ADNLPModels.jl becomes useful.","category":"page"},{"location":"mixed/","page":"Build a hybrid NLPModel","title":"Build a hybrid NLPModel","text":"using ADNLPModels\nnlp = ADNLPModel!(model, gradient_backend = model, jacobian_backend = model)","category":"page"},{"location":"mixed/","page":"Build a hybrid NLPModel","title":"Build a hybrid NLPModel","text":"This would be equivalent to do.","category":"page"},{"location":"mixed/","page":"Build a hybrid NLPModel","title":"Build a hybrid NLPModel","text":"nlp = ADNLPModel!(\n  f,\n  x0,\n  c!,\n  [0.0],\n  [0.0],\n  gradient_backend = model,\n  jacobian_backend = model,\n)","category":"page"},{"location":"mixed/","page":"Build a hybrid NLPModel","title":"Build a hybrid NLPModel","text":"get_adbackend(nlp)","category":"page"},{"location":"mixed/","page":"Build a hybrid NLPModel","title":"Build a hybrid NLPModel","text":"Note that the backends used for the gradient and jacobian are now NLPModel. So, a call to grad on nlp","category":"page"},{"location":"mixed/","page":"Build a hybrid NLPModel","title":"Build a hybrid NLPModel","text":"grad(nlp, x0)","category":"page"},{"location":"mixed/","page":"Build a hybrid NLPModel","title":"Build a hybrid NLPModel","text":"would call grad on model","category":"page"},{"location":"mixed/","page":"Build a hybrid NLPModel","title":"Build a hybrid NLPModel","text":"neval_grad(model)","category":"page"},{"location":"mixed/","page":"Build a hybrid NLPModel","title":"Build a hybrid NLPModel","text":"Moreover, as expected, the ADNLPModel nlp also implements the missing methods, e.g.","category":"page"},{"location":"mixed/","page":"Build a hybrid NLPModel","title":"Build a hybrid NLPModel","text":"jprod(nlp, x0, v)","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [ADNLPModels]","category":"page"},{"location":"reference/#ADNLPModels.ADModelBackend","page":"Reference","title":"ADNLPModels.ADModelBackend","text":"ADModelBackend(gradient_backend, hprod_backend, jprod_backend, jtprod_backend, jacobian_backend, hessian_backend, ghjvprod_backend, hprod_residual_backend, jprod_residual_backend, jtprod_residual_backend, jacobian_residual_backend, hessian_residual_backend)\n\nStructure that define the different backend used to compute automatic differentiation of an ADNLPModel/ADNLSModel model. The different backend are all subtype of ADBackend and are respectively used for:\n\ngradient computation;\nhessian-vector products;\njacobian-vector products;\ntranspose jacobian-vector products;\njacobian computation;\nhessian computation;\ndirectional second derivative computation, i.e. gᵀ ∇²cᵢ(x) v.\n\nThe default constructors are      ADModelBackend(nvar, f, ncon = 0, c = (args...) -> []; showtime::Bool = false, kwargs...)     ADModelNLSBackend(nvar, F!, nequ, ncon = 0, c = (args...) -> []; showtime::Bool = false, kwargs...)\n\nIf show_time is set to true, it prints the time used to generate each backend.\n\nThe remaining kwargs are either the different backends as listed below or arguments passed to the backend's constructors:\n\ngradient_backend = ForwardDiffADGradient;\nhprod_backend = ForwardDiffADHvprod;\njprod_backend = ForwardDiffADJprod;\njtprod_backend = ForwardDiffADJtprod;\njacobian_backend = SparseADJacobian;\nhessian_backend = ForwardDiffADHessian;\nghjvprod_backend = ForwardDiffADGHjvprod;\nhprod_residual_backend = ForwardDiffADHvprod for ADNLSModel and EmptyADbackend otherwise;\njprod_residual_backend = ForwardDiffADJprod for ADNLSModel and EmptyADbackend otherwise;\njtprod_residual_backend = ForwardDiffADJtprod for ADNLSModel and EmptyADbackend otherwise;\njacobian_residual_backend = SparseADJacobian for ADNLSModel and EmptyADbackend otherwise;\nhessian_residual_backend = ForwardDiffADHessian for ADNLSModel and EmptyADbackend otherwise.\n\n\n\n\n\n","category":"type"},{"location":"reference/#ADNLPModels.ADNLPModel-Union{Tuple{S}, Tuple{Any, S}} where S","page":"Reference","title":"ADNLPModels.ADNLPModel","text":"ADNLPModel(f, x0)\nADNLPModel(f, x0, lvar, uvar)\nADNLPModel(f, x0, clinrows, clincols, clinvals, lcon, ucon)\nADNLPModel(f, x0, A, lcon, ucon)\nADNLPModel(f, x0, c, lcon, ucon)\nADNLPModel(f, x0, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLPModel(f, x0, A, c, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, A, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, c, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, A, c, lcon, ucon)\nADNLPModel(model::AbstractNLPModel)\n\nADNLPModel is an AbstractNLPModel using automatic differentiation to compute the derivatives. The problem is defined as\n\n min  f(x)\ns.to  lcon ≤ (  Ax  ) ≤ ucon\n             ( c(x) )\n      lvar ≤   x  ≤ uvar.\n\nThe following keyword arguments are available to all constructors:\n\nminimize: A boolean indicating whether this is a minimization problem (default: true)\nname: The name of the model (default: \"Generic\")\n\nThe following keyword arguments are available to the constructors for constrained problems:\n\ny0: An inital estimate to the Lagrangian multipliers (default: zeros)\n\nADNLPModel uses ForwardDiff and ReverseDiff for the automatic differentiation. One can specify a new backend with the keyword arguments backend::ADNLPModels.ADBackend. There are three pre-coded backends:\n\nthe default ForwardDiffAD.\nReverseDiffAD.\nZygoteDiffAD accessible after loading Zygote.jl in your environment.\n\nFor an advanced usage, one can define its own backend and redefine the API as done in ADNLPModels.jl/src/forward.jl.\n\nExamples\n\nusing ADNLPModels\nf(x) = sum(x)\nx0 = ones(3)\nnvar = 3\nADNLPModel(f, x0) # uses the default ForwardDiffAD backend.\nADNLPModel(f, x0; backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nusing Zygote\nADNLPModel(f, x0; backend = ADNLPModels.ZygoteAD)\n\nusing ADNLPModels\nf(x) = sum(x)\nx0 = ones(3)\nc(x) = [1x[1] + x[2]; x[2]]\nnvar, ncon = 3, 2\nADNLPModel(f, x0, c, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\nADNLPModel(f, x0, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nusing Zygote\nADNLPModel(f, x0, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ZygoteAD)\n\nFor in-place constraints function, use one of the following constructors:\n\nADNLPModel!(f, x0, c!, lcon, ucon)\nADNLPModel!(f, x0, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLPModel!(f, x0, A, c!, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, c!, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, A, c!, lcon, ucon)\nADNLSModel!(model::AbstractNLSModel)\n\nwhere the constraint function has the signature c!(output, input).\n\nusing ADNLPModels\nf(x) = sum(x)\nx0 = ones(3)\nfunction c!(output, x) \n  output[1] = 1x[1] + x[2]\n  output[2] = x[2]\nend\nnvar, ncon = 3, 2\nnlp = ADNLPModel!(f, x0, c!, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.ADNLSModel-Union{Tuple{S}, Tuple{Any, S, Integer}} where S","page":"Reference","title":"ADNLPModels.ADNLSModel","text":"ADNLSModel(F, x0, nequ)\nADNLSModel(F, x0, nequ, lvar, uvar)\nADNLSModel(F, x0, nequ, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel(F, x0, nequ, A, lcon, ucon)\nADNLSModel(F, x0, nequ, c, lcon, ucon)\nADNLSModel(F, x0, nequ, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLSModel(F, x0, nequ, A, c, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, A, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, c, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, A, c, lcon, ucon)\nADNLSModel(model::AbstractNLSModel)\n\nADNLSModel is an Nonlinear Least Squares model using automatic differentiation to compute the derivatives. The problem is defined as\n\n min  ½‖F(x)‖²\ns.to  lcon ≤ (  Ax  ) ≤ ucon\n             ( c(x) )\n      lvar ≤   x  ≤ uvar\n\nwhere nequ is the size of the vector F(x) and the linear constraints come first.\n\nThe following keyword arguments are available to all constructors:\n\nlinequ: An array of indexes of the linear equations (default: Int[])\nminimize: A boolean indicating whether this is a minimization problem (default: true)\nname: The name of the model (default: \"Generic\")\n\nThe following keyword arguments are available to the constructors for constrained problems:\n\ny0: An inital estimate to the Lagrangian multipliers (default: zeros)\n\nADNLSModel uses ForwardDiff and ReverseDiff for the automatic differentiation. One can specify a new backend with the keyword arguments backend::ADNLPModels.ADBackend. There are three pre-coded backends:\n\nthe default ForwardDiffAD.\nReverseDiffAD.\nZygoteDiffAD accessible after loading Zygote.jl in your environment.\n\nFor an advanced usage, one can define its own backend and redefine the API as done in ADNLPModels.jl/src/forward.jl.\n\nExamples\n\nusing ADNLPModels\nF(x) = [x[2]; x[1]]\nnequ = 2\nx0 = ones(3)\nnvar = 3\nADNLSModel(F, x0, nequ) # uses the default ForwardDiffAD backend.\nADNLSModel(F, x0, nequ; backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nusing Zygote\nADNLSModel(F, x0, nequ; backend = ADNLPModels.ZygoteAD)\n\nusing ADNLPModels\nF(x) = [x[2]; x[1]]\nnequ = 2\nx0 = ones(3)\nc(x) = [1x[1] + x[2]; x[2]]\nnvar, ncon = 3, 2\nADNLSModel(F, x0, nequ, c, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\nADNLSModel(F, x0, nequ, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nusing Zygote\nADNLSModel(F, x0, nequ, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ZygoteAD)\n\nFor in-place constraints and residual function, use one of the following constructors:\n\nADNLSModel!(F!, x0, nequ)\nADNLSModel!(F!, x0, nequ, lvar, uvar)\nADNLSModel!(F!, x0, nequ, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel!(F!, x0, nequ, A, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, A, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, A, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, A, clcon, ucon)\nADNLSModel!(model::AbstractNLSModel)\n\nwhere the constraint function has the signature c!(output, input).\n\nusing ADNLPModels\nfunction F!(output, x)\n  output[1] = x[2]\n  output[2] = x[1]\nend\nnequ = 2\nx0 = ones(3)\nfunction c!(output, x) \n  output[1] = 1x[1] + x[2]\n  output[2] = x[2]\nend\nnvar, ncon = 3, 2\nnls = ADNLSModel!(F!, x0, nequ, c!, zeros(ncon), zeros(ncon))\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.compute_hessian_sparsity-NTuple{4, Any}","page":"Reference","title":"ADNLPModels.compute_hessian_sparsity","text":"compute_hessian_sparsity(f, nvar, c!, ncon; detector)\n\nReturn a sparse boolean matrix that represents the adjacency matrix of the Hessian of f(x) + λᵀc(x).\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.compute_jacobian_sparsity","page":"Reference","title":"ADNLPModels.compute_jacobian_sparsity","text":"compute_jacobian_sparsity(c, x0; detector)\ncompute_jacobian_sparsity(c!, cx, x0; detector)\n\nReturn a sparse boolean matrix that represents the adjacency matrix of the Jacobian of c(x).\n\n\n\n\n\n","category":"function"},{"location":"reference/#ADNLPModels.get_F-Tuple{ADNLPModels.AbstractADNLSModel}","page":"Reference","title":"ADNLPModels.get_F","text":"get_F(nls)\nget_F(nls, ::ADBackend)\n\nReturn the out-of-place version of nls.F!.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_adbackend-Tuple{Union{ADNLPModels.AbstractADNLPModel{T, S}, ADNLPModels.AbstractADNLSModel{T, S}} where {T, S}}","page":"Reference","title":"ADNLPModels.get_adbackend","text":"get_adbackend(nlp)\n\nReturns the value adbackend from nlp.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_c-Tuple{Union{ADNLPModels.AbstractADNLPModel{T, S}, ADNLPModels.AbstractADNLSModel{T, S}} where {T, S}}","page":"Reference","title":"ADNLPModels.get_c","text":"get_c(nlp)\nget_c(nlp, ::ADBackend)\n\nReturn the out-of-place version of nlp.c!.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_default_backend-Tuple{Symbol, Vararg{Any}}","page":"Reference","title":"ADNLPModels.get_default_backend","text":"get_default_backend(meth::Symbol, backend::Symbol; kwargs...)\nget_default_backend(::Val{::Symbol}, backend; kwargs...)\n\nReturn a type <:ADBackend that corresponds to the default backend use for the method meth. See keys(ADNLPModels.predefined_backend) for a list of possible backends.\n\nThe following keyword arguments are accepted:\n\nmatrix_free::Bool: If true, this returns an EmptyADbackend for methods that handle matrices, e.g. :hessian_backend.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_lag-Tuple{ADNLPModels.AbstractADNLPModel, ADNLPModels.ADBackend, Real}","page":"Reference","title":"ADNLPModels.get_lag","text":"get_lag(nlp, b::ADBackend, obj_weight)\nget_lag(nlp, b::ADBackend, obj_weight, y)\n\nReturn the lagrangian function ℓ(x) = obj_weight * f(x) + c(x)ᵀy.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_nln_nnzh-Tuple{ADNLPModels.ADModelBackend, Any}","page":"Reference","title":"ADNLPModels.get_nln_nnzh","text":"get_nln_nnzh(::ADBackend, nvar)\nget_nln_nnzh(b::ADModelBackend, nvar)\nget_nln_nnzh(nlp::AbstractNLPModel, nvar)\n\nFor a given ADBackend of a problem with nvar variables, return the number of nonzeros in the lower triangle of the Hessian. If b is the ADModelBackend then b.hessian_backend is used.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_nln_nnzj-Tuple{ADNLPModels.ADModelBackend, Any, Any}","page":"Reference","title":"ADNLPModels.get_nln_nnzj","text":"get_nln_nnzj(::ADBackend, nvar, ncon)\nget_nln_nnzj(b::ADModelBackend, nvar, ncon)\nget_nln_nnzj(nlp::AbstractNLPModel, nvar, ncon)\n\nFor a given ADBackend of a problem with nvar variables and ncon constraints, return the number of nonzeros in the Jacobian of nonlinear constraints. If b is the ADModelBackend then b.jacobian_backend is used.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_residual_nnzh-Tuple{ADNLPModels.ADModelBackend, Any}","page":"Reference","title":"ADNLPModels.get_residual_nnzh","text":"get_residual_nnzh(b::ADModelBackend, nvar)\nget_residual_nnzh(nls::AbstractNLSModel, nvar)\n\nReturn the number of nonzeros elements in the residual Hessians.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_residual_nnzj-Tuple{ADNLPModels.ADModelBackend, Any, Any}","page":"Reference","title":"ADNLPModels.get_residual_nnzj","text":"get_residual_nnzj(b::ADModelBackend, nvar, nequ)\nget_residual_nnzj(nls::AbstractNLSModel, nvar, nequ)\n\nReturn the number of nonzeros elements in the residual Jacobians.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.set_adbackend!-Tuple{Union{ADNLPModels.AbstractADNLPModel{T, S}, ADNLPModels.AbstractADNLSModel{T, S}} where {T, S}, ADNLPModels.ADModelBackend}","page":"Reference","title":"ADNLPModels.set_adbackend!","text":"set_adbackend!(nlp, new_adbackend)\nset_adbackend!(nlp; kwargs...)\n\nReplace the current adbackend value of nlp by new_adbackend or instantiate a new one with kwargs, see ADModelBackend. By default, the setter with kwargs will reuse existing backends.\n\n\n\n\n\n","category":"method"},{"location":"sparse/#Sparse-Hessian-and-Jacobian-computations","page":"Sparse Jacobian and Hessian","title":"Sparse Hessian and Jacobian computations","text":"","category":"section"},{"location":"sparse/","page":"Sparse Jacobian and Hessian","title":"Sparse Jacobian and Hessian","text":"It is to be noted that by default the Jacobian and Hessian are sparse.","category":"page"},{"location":"sparse/","page":"Sparse Jacobian and Hessian","title":"Sparse Jacobian and Hessian","text":"using ADNLPModels, NLPModels\n\nf(x) = (x[1] - 1)^2\nT = Float64\nx0 = T[-1.2; 1.0]\nlvar, uvar = zeros(T, 2), ones(T, 2) # must be of same type than `x0`\nlcon, ucon = -T[0.5], T[0.5]\nc!(cx, x) = begin\n  cx[1] = x[2]\n  return cx\nend\nnlp = ADNLPModel!(f, x0, lvar, uvar, c!, lcon, ucon, backend = :optimized)","category":"page"},{"location":"sparse/","page":"Sparse Jacobian and Hessian","title":"Sparse Jacobian and Hessian","text":"(get_nnzj(nlp), get_nnzh(nlp))  # number of nonzeros elements in the Jacobian and Hessian","category":"page"},{"location":"sparse/","page":"Sparse Jacobian and Hessian","title":"Sparse Jacobian and Hessian","text":"x = rand(T, 2)\nJ = jac(nlp, x)","category":"page"},{"location":"sparse/","page":"Sparse Jacobian and Hessian","title":"Sparse Jacobian and Hessian","text":"x = rand(T, 2)\nH = hess(nlp, x)","category":"page"},{"location":"sparse/","page":"Sparse Jacobian and Hessian","title":"Sparse Jacobian and Hessian","text":"The available backends for sparse derivatives (SparseADJacobian, SparseADHessian and SparseReverseADHessian) have keyword arguments detector and coloring to specify the sparsity pattern detector and the coloring algorithm, respectively.","category":"page"},{"location":"sparse/","page":"Sparse Jacobian and Hessian","title":"Sparse Jacobian and Hessian","text":"Detector: A detector must be of type ADTypes.AbstractSparsityDetector.","category":"page"},{"location":"sparse/","page":"Sparse Jacobian and Hessian","title":"Sparse Jacobian and Hessian","text":"The default detector is TracerSparsityDetector() from the package SparseConnectivityTracer.jl. Prior to version 0.8.0, the default detector was SymbolicSparsityDetector() from Symbolics.jl.","category":"page"},{"location":"sparse/","page":"Sparse Jacobian and Hessian","title":"Sparse Jacobian and Hessian","text":"Coloring: A coloring must be of type ADTypes.AbstractColoringAlgorithm.","category":"page"},{"location":"sparse/","page":"Sparse Jacobian and Hessian","title":"Sparse Jacobian and Hessian","text":"The default algorithm is GreedyColoringAlgorithm() from the package SparseMatrixColorings.jl.","category":"page"},{"location":"sparse/","page":"Sparse Jacobian and Hessian","title":"Sparse Jacobian and Hessian","text":"The package SparseConnectivityTracer.jl is used to compute the sparsity pattern of Jacobians and Hessians. The evaluation of the number of directional derivatives and the seeds required to compute compressed Jacobians and Hessians is performed using SparseMatrixColorings.jl. As of release v0.8.1, it has replaced ColPack.jl. We acknowledge Guillaume Dalle (@gdalle), Adrian Hill (@adrhill), and Michel Schanen (@michel2323) for the development of these packages.","category":"page"},{"location":"#ADNLPModels","page":"Home","title":"ADNLPModels","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides automatic differentiation (AD)-based model implementations that conform to the NLPModels API. The general form of the optimization problem is","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginaligned\nmin quad  f(x) \n c_L leq c(x) leq c_U \n ell leq x leq u\nendaligned","category":"page"},{"location":"#Install","page":"Home","title":"Install","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ADNLPModels Julia Language package. To install ADNLPModels, please open Julia's interactive session (known as REPL) and press the ] key in the REPL to use the package mode, then type the following command","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add ADNLPModels","category":"page"},{"location":"#Complementary-packages","page":"Home","title":"Complementary packages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ADNLPModels.jl functionalities are extended by other packages that are not automatically loaded. In other words, you sometimes need to load the desired package separately to access some functionalities.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using ADNLPModels # load only the default functionalities\nusing Zygote # load the Zygote backends","category":"page"},{"location":"","page":"Home","title":"Home","text":"Versions compatibility for the extensions are available in the file test/Project.toml.","category":"page"},{"location":"","page":"Home","title":"Home","text":"print(open(io->read(io, String), \"../../test/Project.toml\"))","category":"page"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package defines two models, ADNLPModel for general nonlinear optimization, and ADNLSModel for nonlinear least-squares problems.","category":"page"},{"location":"","page":"Home","title":"Home","text":"ADNLPModel\nADNLSModel","category":"page"},{"location":"#ADNLPModels.ADNLPModel","page":"Home","title":"ADNLPModels.ADNLPModel","text":"ADNLPModel(f, x0)\nADNLPModel(f, x0, lvar, uvar)\nADNLPModel(f, x0, clinrows, clincols, clinvals, lcon, ucon)\nADNLPModel(f, x0, A, lcon, ucon)\nADNLPModel(f, x0, c, lcon, ucon)\nADNLPModel(f, x0, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLPModel(f, x0, A, c, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, A, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, c, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, A, c, lcon, ucon)\nADNLPModel(model::AbstractNLPModel)\n\nADNLPModel is an AbstractNLPModel using automatic differentiation to compute the derivatives. The problem is defined as\n\n min  f(x)\ns.to  lcon ≤ (  Ax  ) ≤ ucon\n             ( c(x) )\n      lvar ≤   x  ≤ uvar.\n\nThe following keyword arguments are available to all constructors:\n\nminimize: A boolean indicating whether this is a minimization problem (default: true)\nname: The name of the model (default: \"Generic\")\n\nThe following keyword arguments are available to the constructors for constrained problems:\n\ny0: An inital estimate to the Lagrangian multipliers (default: zeros)\n\nADNLPModel uses ForwardDiff and ReverseDiff for the automatic differentiation. One can specify a new backend with the keyword arguments backend::ADNLPModels.ADBackend. There are three pre-coded backends:\n\nthe default ForwardDiffAD.\nReverseDiffAD.\nZygoteDiffAD accessible after loading Zygote.jl in your environment.\n\nFor an advanced usage, one can define its own backend and redefine the API as done in ADNLPModels.jl/src/forward.jl.\n\nExamples\n\nusing ADNLPModels\nf(x) = sum(x)\nx0 = ones(3)\nnvar = 3\nADNLPModel(f, x0) # uses the default ForwardDiffAD backend.\nADNLPModel(f, x0; backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nusing Zygote\nADNLPModel(f, x0; backend = ADNLPModels.ZygoteAD)\n\nusing ADNLPModels\nf(x) = sum(x)\nx0 = ones(3)\nc(x) = [1x[1] + x[2]; x[2]]\nnvar, ncon = 3, 2\nADNLPModel(f, x0, c, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\nADNLPModel(f, x0, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nusing Zygote\nADNLPModel(f, x0, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ZygoteAD)\n\nFor in-place constraints function, use one of the following constructors:\n\nADNLPModel!(f, x0, c!, lcon, ucon)\nADNLPModel!(f, x0, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLPModel!(f, x0, A, c!, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, c!, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, A, c!, lcon, ucon)\nADNLSModel!(model::AbstractNLSModel)\n\nwhere the constraint function has the signature c!(output, input).\n\nusing ADNLPModels\nf(x) = sum(x)\nx0 = ones(3)\nfunction c!(output, x) \n  output[1] = 1x[1] + x[2]\n  output[2] = x[2]\nend\nnvar, ncon = 3, 2\nnlp = ADNLPModel!(f, x0, c!, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\n\n\n\n\n\n","category":"type"},{"location":"#ADNLPModels.ADNLSModel","page":"Home","title":"ADNLPModels.ADNLSModel","text":"ADNLSModel(F, x0, nequ)\nADNLSModel(F, x0, nequ, lvar, uvar)\nADNLSModel(F, x0, nequ, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel(F, x0, nequ, A, lcon, ucon)\nADNLSModel(F, x0, nequ, c, lcon, ucon)\nADNLSModel(F, x0, nequ, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLSModel(F, x0, nequ, A, c, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, A, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, c, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, A, c, lcon, ucon)\nADNLSModel(model::AbstractNLSModel)\n\nADNLSModel is an Nonlinear Least Squares model using automatic differentiation to compute the derivatives. The problem is defined as\n\n min  ½‖F(x)‖²\ns.to  lcon ≤ (  Ax  ) ≤ ucon\n             ( c(x) )\n      lvar ≤   x  ≤ uvar\n\nwhere nequ is the size of the vector F(x) and the linear constraints come first.\n\nThe following keyword arguments are available to all constructors:\n\nlinequ: An array of indexes of the linear equations (default: Int[])\nminimize: A boolean indicating whether this is a minimization problem (default: true)\nname: The name of the model (default: \"Generic\")\n\nThe following keyword arguments are available to the constructors for constrained problems:\n\ny0: An inital estimate to the Lagrangian multipliers (default: zeros)\n\nADNLSModel uses ForwardDiff and ReverseDiff for the automatic differentiation. One can specify a new backend with the keyword arguments backend::ADNLPModels.ADBackend. There are three pre-coded backends:\n\nthe default ForwardDiffAD.\nReverseDiffAD.\nZygoteDiffAD accessible after loading Zygote.jl in your environment.\n\nFor an advanced usage, one can define its own backend and redefine the API as done in ADNLPModels.jl/src/forward.jl.\n\nExamples\n\nusing ADNLPModels\nF(x) = [x[2]; x[1]]\nnequ = 2\nx0 = ones(3)\nnvar = 3\nADNLSModel(F, x0, nequ) # uses the default ForwardDiffAD backend.\nADNLSModel(F, x0, nequ; backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nusing Zygote\nADNLSModel(F, x0, nequ; backend = ADNLPModels.ZygoteAD)\n\nusing ADNLPModels\nF(x) = [x[2]; x[1]]\nnequ = 2\nx0 = ones(3)\nc(x) = [1x[1] + x[2]; x[2]]\nnvar, ncon = 3, 2\nADNLSModel(F, x0, nequ, c, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\nADNLSModel(F, x0, nequ, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nusing Zygote\nADNLSModel(F, x0, nequ, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ZygoteAD)\n\nFor in-place constraints and residual function, use one of the following constructors:\n\nADNLSModel!(F!, x0, nequ)\nADNLSModel!(F!, x0, nequ, lvar, uvar)\nADNLSModel!(F!, x0, nequ, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel!(F!, x0, nequ, A, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, A, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, A, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, A, clcon, ucon)\nADNLSModel!(model::AbstractNLSModel)\n\nwhere the constraint function has the signature c!(output, input).\n\nusing ADNLPModels\nfunction F!(output, x)\n  output[1] = x[2]\n  output[2] = x[1]\nend\nnequ = 2\nx0 = ones(3)\nfunction c!(output, x) \n  output[1] = 1x[1] + x[2]\n  output[2] = x[2]\nend\nnvar, ncon = 3, 2\nnls = ADNLSModel!(F!, x0, nequ, c!, zeros(ncon), zeros(ncon))\n\n\n\n\n\n","category":"type"},{"location":"","page":"Home","title":"Home","text":"Check the Tutorial for more details on the usage.","category":"page"},{"location":"#License","page":"Home","title":"License","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This content is released under the MPL2.0 License.","category":"page"},{"location":"#Bug-reports-and-discussions","page":"Home","title":"Bug reports and discussions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you think you found a bug, feel free to open an issue. Focused suggestions and requests can also be opened as issues. Before opening a pull request, start an issue or a discussion on the topic, please.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If you want to ask a question not suited for a bug report, feel free to start a discussion here. This forum is for general discussion about this repository and the JuliaSmoothOptimizers, so questions about any of our packages are welcome.","category":"page"},{"location":"#Contents","page":"Home","title":"Contents","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Pages = [\"tutorial.md\"]","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"You can check an Introduction to ADNLPModels.jl on our site, jso.dev.","category":"page"},{"location":"generic/#Creating-an-ADNLPModels-backend-that-supports-multiple-precisions","page":"Support multiple precision","title":"Creating an ADNLPModels backend that supports multiple precisions","text":"","category":"section"},{"location":"generic/","page":"Support multiple precision","title":"Support multiple precision","text":"Pages = [\"generic.md\"]","category":"page"},{"location":"generic/","page":"Support multiple precision","title":"Support multiple precision","text":"You can check the tutorial Creating an ADNLPModels backend that supports multiple precisions on our site, jso.dev.","category":"page"}]
}
