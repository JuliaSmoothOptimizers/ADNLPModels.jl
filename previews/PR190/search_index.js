var documenterSearchIndex = {"docs":
[{"location":"backend/#How-to-switch-backend-in-ADNLPModels","page":"Backend","title":"How to switch backend in ADNLPModels","text":"","category":"section"},{"location":"backend/","page":"Backend","title":"Backend","text":"ADNLPModels allows the use of different backends to compute the derivatives required within NLPModel API. It uses ForwardDiff.jl, ReverseDiff.jl, and more via optional depencies.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"The backend information is in a structure ADNLPModels.ADModelBackend in the attribute adbackend of a ADNLPModel, it can also be accessed with get_adbackend.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"The functions used internally to define the NLPModel API and the possible backends are defined in the following table:","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"Functions FowardDiff backends ReverseDiff backends Zygote backends Enzyme backend SparseDiffTools backend Symbolics backend\ngradient and gradient! ForwardDiffADGradient/GenericForwardDiffADGradient ReverseDiffADGradient/GenericReverseDiffADGradient ZygoteADGradient EnzymeADGradient – –\njacobian ForwardDiffADJacobian ReverseDiffADJacobian ZygoteADJacobian – SDTSparseADJacobian SparseADJacobian/SparseSymbolicsADJacobian\nhessian ForwardDiffADHessian ReverseDiffADHessian ZygoteADHessian – – SparseADHessian/SparseSymbolicsADHessian\nJprod ForwardDiffADJprod/GenericForwardDiffADJprod ReverseDiffADJprod/GenericReverseDiffADJprod ZygoteADJprod – SDTForwardDiffADJprod –\nJtprod ForwardDiffADJtprod/GenericForwardDiffADJtprod ReverseDiffADJtprod/GenericReverseDiffADJtprod ZygoteADJtprod – – –\nHvprod ForwardDiffADHvprod/GenericForwardDiffADHvprod ReverseDiffADHvprod/GenericReverseDiffADHvprod – – SDTForwardDiffADHvprod –\ndirectional_second_derivative ForwardDiffADGHjvprod – – – – ","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"The functions hess_structure!, hess_coord!, jac_structure! and jac_coord! defined in ad.jl are generic to all the backends for now.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"using ADNLPModels\nf(x) = sum(x)\nx0 = ones(2)\nADNLPModel(f, x0, show_time = true)","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"The keyword show_time is set to true to display the time needed to instantiate each backend. For unconstrained problem, there is no need to compute derivatives of constraints so an EmptyADbackend is used for Jacobian computations.","category":"page"},{"location":"backend/#Examples","page":"Backend","title":"Examples","text":"","category":"section"},{"location":"backend/","page":"Backend","title":"Backend","text":"We now present a serie of practical examples. For simplicity, we focus here on unconstrained optimization problem. All these examples can be generalized to problems with bounds, constraints or nonlinear least-squares.","category":"page"},{"location":"backend/#Use-another-backend","page":"Backend","title":"Use another backend","text":"","category":"section"},{"location":"backend/","page":"Backend","title":"Backend","text":"As shown in Tutorial, it is very straightforward to instantiate an ADNLPModel using an objective function and an initial guess.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"using ADNLPModels, NLPModels\nf(x) = sum(x)\nx0 = ones(3)\nnlp = ADNLPModel(f, x0)\ngrad(nlp, nlp.meta.x0) # returns the gradient at x0","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"Thanks to the backends inside ADNLPModels.jl, it is easy to change the backend for one (or more) function using the kwargs presented in the table above.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"nlp = ADNLPModel(f, x0, gradient_backend = ADNLPModels.ReverseDiffADGradient)\ngrad(nlp, nlp.meta.x0) # returns the gradient at x0 using `ReverseDiff`","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"It is also possible to try some new implementation for each function. First, we define a new ADBackend structure.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"struct NewADGradient <: ADNLPModels.ADBackend end\nfunction NewADGradient(\n  nvar::Integer,\n  f,\n  ncon::Integer = 0,\n  c::Function = (args...) -> [];\n  kwargs...,\n)\n  return NewADGradient()\nend","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"Then, we implement the desired functions following the table above.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"ADNLPModels.gradient(adbackend::NewADGradient, f, x) = rand(Float64, size(x))\nfunction ADNLPModels.gradient!(adbackend::NewADGradient, g, f, x)\n  g .= rand(Float64, size(x))\n  return g\nend","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"Finally, we use the homemade backend to compute the gradient.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"nlp = ADNLPModel(sum, ones(3), gradient_backend = NewADGradient)\ngrad(nlp, nlp.meta.x0) # returns the gradient at x0 using `NewADGradient`","category":"page"},{"location":"backend/#Change-backend","page":"Backend","title":"Change backend","text":"","category":"section"},{"location":"backend/","page":"Backend","title":"Backend","text":"Once an instance of an ADNLPModel has been created, it is possible to change the backends without re-instantiating the model.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"using ADNLPModels, NLPModels\nf(x) = 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = 3 * ones(2)\nnlp = ADNLPModel(f, x0)\nget_adbackend(nlp) # returns the `ADModelBackend` structure that regroup all the various backends.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"There are currently two ways to modify instantiated backends. The first one is to instantiate a new ADModelBackend and use set_adbackend! to modify nlp.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"adback = ADNLPModels.ADModelBackend(nlp.meta.nvar, nlp.f, gradient_backend = ADNLPModels.ForwardDiffADGradient)\nset_adbackend!(nlp, adback)\nget_adbackend(nlp)","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"The alternative is to use `set_adbackend! and pass the new backends via kwargs. In the second approach, it is possible to pass either the type of the desired backend or an instance as shown below.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"set_adbackend!(\n  nlp,\n  gradient_backend = ADNLPModels.ForwardDiffADGradient,\n  jtprod_backend = ADNLPModels.GenericForwardDiffADJtprod(),\n)\nget_adbackend(nlp)","category":"page"},{"location":"backend/#Support-multiple-precision-without-having-to-recreate-the-model","page":"Backend","title":"Support multiple precision without having to recreate the model","text":"","category":"section"},{"location":"backend/","page":"Backend","title":"Backend","text":"One of the strength of ADNLPModels.jl is the type flexibility. Let's assume, we first instantiate an ADNLPModel with a Float64 initial guess.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"using ADNLPModels, NLPModels\nf(x) = 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = 3 * ones(2) # Float64 initial guess\nnlp = ADNLPModel(f, x0)","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"Then, the gradient will return a vector of Float64.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"x64 = rand(2)\ngrad(nlp, x64)","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"It is now possible to move to a different type, for instance Float32, while keeping the instance nlp.","category":"page"},{"location":"backend/","page":"Backend","title":"Backend","text":"x0_32 = ones(Float32, 2)\nset_adbackend!(nlp, gradient_backend = ADNLPModels.ForwardDiffADGradient, x0 = x0_32)\nx32 = rand(Float32, 2)\ngrad(nlp, x32)","category":"page"},{"location":"predefined/#Default-backend-and-performance-in-ADNLPModels","page":"Default backends","title":"Default backend and performance in ADNLPModels","text":"","category":"section"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"As illustrated in the tutorial on backends, ADNLPModels.jl use different backend for each method from the NLPModel API that are implemented. By default, it uses the following:","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"using ADNLPModels, NLPModels\n\nf(x) = 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nT = Float64\nx0 = T[-1.2; 1.0]\nlvar, uvar = zeros(T, 2), ones(T, 2) # must be of same type than `x0`\nlcon, ucon = -T[0.5], T[0.5]\nc!(cx, x) = begin\n  cx[1] = x[1] + x[2]\n  return cx\nend\nnlp = ADNLPModel!(f, x0, lvar, uvar, c!, lcon, ucon)\nget_adbackend(nlp)","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"Note that ForwardDiff.jl is mainly used as it is efficient and stable.","category":"page"},{"location":"predefined/#Predefined-backends","page":"Default backends","title":"Predefined backends","text":"","category":"section"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"Another way to know the default backends used is to check the constant ADNLPModels.default_backend.","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"ADNLPModels.default_backend","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"More generally, the package anticipates more uses","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"ADNLPModels.predefined_backend","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"The backend optimized will mainly focus on the most efficient approaches, for instance using ReverseDiff to compute the gradient instead of ForwardDiff.","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"ADNLPModels.predefined_backend[:optimized]","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"The backend generic focuses on backend that make no assumptions on the element type, see Creating an ADNLPModels backend that supports multiple precisions.","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"It is possible to use these pre-defined backends using the keyword argument backend when instantiating the model.","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"nlp = ADNLPModel!(f, x0, lvar, uvar, c!, lcon, ucon, backend = :optimized)\nget_adbackend(nlp)","category":"page"},{"location":"predefined/#Hessian-and-Jacobian-computations","page":"Default backends","title":"Hessian and Jacobian computations","text":"","category":"section"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"It is to be noted that by default the Jacobian and Hessian matrices are dense.","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"(get_nnzj(nlp), get_nnzh(nlp)) # number of nonzeros elements in the Jacobian and Hessian","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"To enable sparse computations of these entries, one needs to first load the package Symbolics.jl.","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"using Symbolics","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"and now","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"ADNLPModels.predefined_backend[:optimized][:jacobian_backend]","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"ADNLPModels.predefined_backend[:optimized][:hessian_backend]","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"So, taking another optimization problem with the optimized backend will compute sparse Jacobian and Hessian matrices.","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"f(x) = (x[1] - 1)^2\nT = Float64\nx0 = T[-1.2; 1.0]\nlvar, uvar = zeros(T, 2), ones(T, 2) # must be of same type than `x0`\nlcon, ucon = -T[0.5], T[0.5]\nc!(cx, x) = begin\n  cx[1] = x[2]\n  return cx\nend\nnlp = ADNLPModel!(f, x0, lvar, uvar, c!, lcon, ucon, backend = :optimized)\n(get_nnzj(nlp), get_nnzh(nlp))","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"x = rand(T, 2)\njac(nlp, x)","category":"page"},{"location":"predefined/","page":"Default backends","title":"Default backends","text":"The package Symbolics.jl is used to compute the sparsity pattern of the sparse matrix. The evaluation of the number of directional derivatives needed to evaluate the matrix is done by ColPack.jl.","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [ADNLPModels]","category":"page"},{"location":"reference/#ADNLPModels.ADModelBackend","page":"Reference","title":"ADNLPModels.ADModelBackend","text":"ADModelBackend(gradient_backend, hprod_backend, jprod_backend, jtprod_backend, jacobian_backend, hessian_backend, ghjvprod_backend, hprod_residual_backend, jprod_residual_backend, jtprod_residual_backend, jacobian_residual_backend, hessian_residual_backend)\n\nStructure that define the different backend used to compute automatic differentiation of an ADNLPModel/ADNLSModel model. The different backend are all subtype of ADBackend and are respectively used for:\n\ngradient computation;\nhessian-vector products;\njacobian-vector products;\ntranspose jacobian-vector products;\njacobian computation;\nhessian computation;\ndirectional second derivative computation, i.e. gᵀ ∇²cᵢ(x) v.\n\nThe default constructors are      ADModelBackend(nvar, f, ncon = 0, c = (args...) -> []; showtime::Bool = false, kwargs...)     ADModelNLSBackend(nvar, F!, nequ, ncon = 0, c = (args...) -> []; showtime::Bool = false, kwargs...)\n\nIf show_time is set to true, it prints the time used to generate each backend.\n\nThe remaining kwargs are either the different backends as listed below or arguments passed to the backend's constructors:\n\ngradient_backend = ForwardDiffADGradient;\nhprod_backend = ForwardDiffADHvprod;\njprod_backend = ForwardDiffADJprod;\njtprod_backend = ForwardDiffADJtprod;\njacobian_backend = SparseADJacobian;\nhessian_backend = ForwardDiffADHessian;\nghjvprod_backend = ForwardDiffADGHjvprod;\nhprod_residual_backend = ForwardDiffADHvprod for ADNLSModel and EmptyADbackend otherwise;\njprod_residual_backend = ForwardDiffADJprod for ADNLSModel and EmptyADbackend otherwise;\njtprod_residual_backend = ForwardDiffADJtprod for ADNLSModel and EmptyADbackend otherwise;\njacobian_residual_backend = SparseADJacobian for ADNLSModel and EmptyADbackend otherwise;\nhessian_residual_backend = ForwardDiffADHessian for ADNLSModel and EmptyADbackend otherwise.\n\n\n\n\n\n","category":"type"},{"location":"reference/#ADNLPModels.ADNLPModel-Union{Tuple{S}, Tuple{Any, S}} where S","page":"Reference","title":"ADNLPModels.ADNLPModel","text":"ADNLPModel(f, x0)\nADNLPModel(f, x0, lvar, uvar)\nADNLPModel(f, x0, clinrows, clincols, clinvals, lcon, ucon)\nADNLPModel(f, x0, A, lcon, ucon)\nADNLPModel(f, x0, c, lcon, ucon)\nADNLPModel(f, x0, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLPModel(f, x0, A, c, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, A, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, c, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, A, c, lcon, ucon)\n\nADNLPModel is an AbstractNLPModel using automatic differentiation to compute the derivatives. The problem is defined as\n\n min  f(x)\ns.to  lcon ≤ (  Ax  ) ≤ ucon\n             ( c(x) )\n      lvar ≤   x  ≤ uvar.\n\nThe following keyword arguments are available to all constructors:\n\nminimize: A boolean indicating whether this is a minimization problem (default: true)\nname: The name of the model (default: \"Generic\")\n\nThe following keyword arguments are available to the constructors for constrained problems:\n\ny0: An inital estimate to the Lagrangian multipliers (default: zeros)\n\nADNLPModel uses ForwardDiff and ReverseDiff for the automatic differentiation. One can specify a new backend with the keyword arguments backend::ADNLPModels.ADBackend. There are three pre-coded backends:\n\nthe default ForwardDiffAD.\nReverseDiffAD.\nZygoteDiffAD accessible after loading Zygote.jl in your environment.\n\nFor an advanced usage, one can define its own backend and redefine the API as done in ADNLPModels.jl/src/forward.jl.\n\nExamples\n\nusing ADNLPModels\nf(x) = sum(x)\nx0 = ones(3)\nnvar = 3\nADNLPModel(f, x0) # uses the default ForwardDiffAD backend.\nADNLPModel(f, x0; backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nusing Zygote\nADNLPModel(f, x0; backend = ADNLPModels.ZygoteAD)\n\nusing ADNLPModels\nf(x) = sum(x)\nx0 = ones(3)\nc(x) = [1x[1] + x[2]; x[2]]\nnvar, ncon = 3, 2\nADNLPModel(f, x0, c, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\nADNLPModel(f, x0, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nusing Zygote\nADNLPModel(f, x0, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ZygoteAD)\n\nFor in-place constraints function, use one of the following constructors:\n\nADNLPModel!(f, x0, c!, lcon, ucon)\nADNLPModel!(f, x0, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLPModel!(f, x0, A, c!, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, c!, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, A, c!, lcon, ucon)\n\nwhere the constraint function has the signature c!(output, input).\n\nusing ADNLPModels\nf(x) = sum(x)\nx0 = ones(3)\nfunction c!(output, x) \n  output[1] = 1x[1] + x[2]\n  output[2] = x[2]\nend\nnvar, ncon = 3, 2\nnlp = ADNLPModel!(f, x0, c!, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.ADNLSModel-Union{Tuple{S}, Tuple{Any, S, Integer}} where S","page":"Reference","title":"ADNLPModels.ADNLSModel","text":"ADNLSModel(F, x0, nequ)\nADNLSModel(F, x0, nequ, lvar, uvar)\nADNLSModel(F, x0, nequ, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel(F, x0, nequ, A, lcon, ucon)\nADNLSModel(F, x0, nequ, c, lcon, ucon)\nADNLSModel(F, x0, nequ, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLSModel(F, x0, nequ, A, c, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, A, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, c, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, A, c, lcon, ucon)\n\nADNLSModel is an Nonlinear Least Squares model using automatic differentiation to compute the derivatives. The problem is defined as\n\n min  ½‖F(x)‖²\ns.to  lcon ≤ (  Ax  ) ≤ ucon\n             ( c(x) )\n      lvar ≤   x  ≤ uvar\n\nwhere nequ is the size of the vector F(x) and the linear constraints come first.\n\nThe following keyword arguments are available to all constructors:\n\nlinequ: An array of indexes of the linear equations (default: Int[])\nminimize: A boolean indicating whether this is a minimization problem (default: true)\nname: The name of the model (default: \"Generic\")\n\nThe following keyword arguments are available to the constructors for constrained problems:\n\ny0: An inital estimate to the Lagrangian multipliers (default: zeros)\n\nADNLSModel uses ForwardDiff and ReverseDiff for the automatic differentiation. One can specify a new backend with the keyword arguments backend::ADNLPModels.ADBackend. There are three pre-coded backends:\n\nthe default ForwardDiffAD.\nReverseDiffAD.\nZygoteDiffAD accessible after loading Zygote.jl in your environment.\n\nFor an advanced usage, one can define its own backend and redefine the API as done in ADNLPModels.jl/src/forward.jl.\n\nExamples\n\nusing ADNLPModels\nF(x) = [x[2]; x[1]]\nnequ = 2\nx0 = ones(3)\nnvar = 3\nADNLSModel(F, x0, nequ) # uses the default ForwardDiffAD backend.\nADNLSModel(F, x0, nequ; backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nusing Zygote\nADNLSModel(F, x0, nequ; backend = ADNLPModels.ZygoteAD)\n\nusing ADNLPModels\nF(x) = [x[2]; x[1]]\nnequ = 2\nx0 = ones(3)\nc(x) = [1x[1] + x[2]; x[2]]\nnvar, ncon = 3, 2\nADNLSModel(F, x0, nequ, c, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\nADNLSModel(F, x0, nequ, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nusing Zygote\nADNLSModel(F, x0, nequ, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ZygoteAD)\n\nFor in-place constraints and residual function, use one of the following constructors:\n\nADNLSModel!(F!, x0, nequ)\nADNLSModel!(F!, x0, nequ, lvar, uvar)\nADNLSModel!(F!, x0, nequ, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel!(F!, x0, nequ, A, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, A, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, A, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, A, clcon, ucon)\n\nwhere the constraint function has the signature c!(output, input).\n\nusing ADNLPModels\nfunction F!(output, x)\n  output[1] = x[2]\n  output[2] = x[1]\nend\nnequ = 2\nx0 = ones(3)\nfunction c!(output, x) \n  output[1] = 1x[1] + x[2]\n  output[2] = x[2]\nend\nnvar, ncon = 3, 2\nnls = ADNLSModel!(F!, x0, nequ, c!, zeros(ncon), zeros(ncon))\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.compute_hessian_sparsity-Tuple","page":"Reference","title":"ADNLPModels.compute_hessian_sparsity","text":"compute_hessian_sparsity(f, nvar, c!, ncon)\n\nReturn a sparse matrix.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.compute_jacobian_sparsity-Tuple","page":"Reference","title":"ADNLPModels.compute_jacobian_sparsity","text":"compute_jacobian_sparsity(c!, cx, x0)\n\nReturn a sparse matrix.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_F-Tuple{ADNLPModels.AbstractADNLSModel}","page":"Reference","title":"ADNLPModels.get_F","text":"get_F(nls)\nget_F(nls, ::ADBackend)\n\nReturn the out-of-place version of nls.F!.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_adbackend-Tuple{Union{ADNLPModels.AbstractADNLPModel{T, S}, ADNLPModels.AbstractADNLSModel{T, S}} where {T, S}}","page":"Reference","title":"ADNLPModels.get_adbackend","text":"get_adbackend(nlp)\n\nReturns the value adbackend from nlp.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_c-Tuple{Union{ADNLPModels.AbstractADNLPModel{T, S}, ADNLPModels.AbstractADNLSModel{T, S}} where {T, S}}","page":"Reference","title":"ADNLPModels.get_c","text":"get_c(nlp)\nget_c(nlp, ::ADBackend)\n\nReturn the out-of-place version of nlp.c!.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_default_backend-Tuple{Symbol, Vararg{Any}}","page":"Reference","title":"ADNLPModels.get_default_backend","text":"get_default_backend(meth::Symbol, backend::Symbol; kwargs...)\nget_default_backend(::Val{::Symbol}, backend; kwargs...)\n\nReturn a type <:ADBackend that corresponds to the default backend use for the method meth. See keys(ADNLPModels.predefined_backend) for a list of possible backends.\n\nThe following keyword arguments are accepted:\n\nmatrix_free::Bool: If true, this returns an EmptyADbackend for methods that handle matrices, e.g. :hessian_backend.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_lag-Tuple{ADNLPModels.AbstractADNLPModel, ADNLPModels.ADBackend, Real}","page":"Reference","title":"ADNLPModels.get_lag","text":"get_lag(nlp, b::ADBackend, obj_weight)\nget_lag(nlp, b::ADBackend, obj_weight, y)\n\nReturn the lagrangian function ℓ(x) = obj_weight * f(x) + c(x)ᵀy.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_nln_nnzh-Tuple{ADNLPModels.ADModelBackend, Any}","page":"Reference","title":"ADNLPModels.get_nln_nnzh","text":"get_nln_nnzh(::ADBackend, nvar)\nget_nln_nnzh(b::ADModelBackend, nvar)\nget_nln_nnzh(nlp::AbstractNLPModel, nvar)\n\nFor a given ADBackend of a problem with nvar variables, return the number of nonzeros in the lower triangle of the Hessian. If b is the ADModelBackend then b.hessian_backend is used.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_nln_nnzj-Tuple{ADNLPModels.ADModelBackend, Any, Any}","page":"Reference","title":"ADNLPModels.get_nln_nnzj","text":"get_nln_nnzj(::ADBackend, nvar, ncon)\nget_nln_nnzj(b::ADModelBackend, nvar, ncon)\nget_nln_nnzj(nlp::AbstractNLPModel, nvar, ncon)\n\nFor a given ADBackend of a problem with nvar variables and ncon constraints, return the number of nonzeros in the Jacobian of nonlinear constraints. If b is the ADModelBackend then b.jacobian_backend is used.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_residual_nnzj-Tuple{ADNLPModels.ADModelBackend, Any, Any}","page":"Reference","title":"ADNLPModels.get_residual_nnzj","text":"get_residual_nnzj(b::ADModelBackend, nvar, nequ)\n\nReturn get_nln_nnzj(b.jacobian_residual_backend, nvar, nequ).\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.set_adbackend!-Tuple{Union{ADNLPModels.AbstractADNLPModel{T, S}, ADNLPModels.AbstractADNLSModel{T, S}} where {T, S}, ADNLPModels.ADModelBackend}","page":"Reference","title":"ADNLPModels.set_adbackend!","text":"set_adbackend!(nlp, new_adbackend)\nset_adbackend!(nlp; kwargs...)\n\nReplace the current adbackend value of nlp by new_adbackend or instantiate a new one with kwargs, see ADModelBackend. By default, the setter with kwargs will reuse existing backends.\n\n\n\n\n\n","category":"method"},{"location":"#ADNLPModels","page":"Home","title":"ADNLPModels","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides automatic differentiation (AD)-based model implementations that conform to the NLPModels API. The general form of the optimization problem is","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginaligned\nmin quad  f(x) \n c_L leq c(x) leq c_U \n ell leq x leq u\nendaligned","category":"page"},{"location":"#Install","page":"Home","title":"Install","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ADNLPModels Julia Language package. To install ADNLPModels, please open Julia's interactive session (known as REPL) and press the ] key in the REPL to use the package mode, then type the following command","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add ADNLPModels","category":"page"},{"location":"#Complementary-packages","page":"Home","title":"Complementary packages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ADNLPModels.jl functionalities are extended by other packages that are not automatically loaded. In other words, you sometimes need to load the desired package separately to access some functionalities.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using ADNLPModels # load only the default functionalities\nusing Zygote # load the Zygote backends","category":"page"},{"location":"","page":"Home","title":"Home","text":"Versions compatibility for the extensions are available in the file test/Project.toml.","category":"page"},{"location":"","page":"Home","title":"Home","text":"print(open(io->read(io, String), \"../../test/Project.toml\"))","category":"page"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package defines two models, ADNLPModel for general nonlinear optimization, and ADNLSModel for nonlinear least-squares problems.","category":"page"},{"location":"","page":"Home","title":"Home","text":"ADNLPModel\nADNLSModel","category":"page"},{"location":"#ADNLPModels.ADNLPModel","page":"Home","title":"ADNLPModels.ADNLPModel","text":"ADNLPModel(f, x0)\nADNLPModel(f, x0, lvar, uvar)\nADNLPModel(f, x0, clinrows, clincols, clinvals, lcon, ucon)\nADNLPModel(f, x0, A, lcon, ucon)\nADNLPModel(f, x0, c, lcon, ucon)\nADNLPModel(f, x0, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLPModel(f, x0, A, c, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, A, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, c, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, A, c, lcon, ucon)\n\nADNLPModel is an AbstractNLPModel using automatic differentiation to compute the derivatives. The problem is defined as\n\n min  f(x)\ns.to  lcon ≤ (  Ax  ) ≤ ucon\n             ( c(x) )\n      lvar ≤   x  ≤ uvar.\n\nThe following keyword arguments are available to all constructors:\n\nminimize: A boolean indicating whether this is a minimization problem (default: true)\nname: The name of the model (default: \"Generic\")\n\nThe following keyword arguments are available to the constructors for constrained problems:\n\ny0: An inital estimate to the Lagrangian multipliers (default: zeros)\n\nADNLPModel uses ForwardDiff and ReverseDiff for the automatic differentiation. One can specify a new backend with the keyword arguments backend::ADNLPModels.ADBackend. There are three pre-coded backends:\n\nthe default ForwardDiffAD.\nReverseDiffAD.\nZygoteDiffAD accessible after loading Zygote.jl in your environment.\n\nFor an advanced usage, one can define its own backend and redefine the API as done in ADNLPModels.jl/src/forward.jl.\n\nExamples\n\nusing ADNLPModels\nf(x) = sum(x)\nx0 = ones(3)\nnvar = 3\nADNLPModel(f, x0) # uses the default ForwardDiffAD backend.\nADNLPModel(f, x0; backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nusing Zygote\nADNLPModel(f, x0; backend = ADNLPModels.ZygoteAD)\n\nusing ADNLPModels\nf(x) = sum(x)\nx0 = ones(3)\nc(x) = [1x[1] + x[2]; x[2]]\nnvar, ncon = 3, 2\nADNLPModel(f, x0, c, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\nADNLPModel(f, x0, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nusing Zygote\nADNLPModel(f, x0, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ZygoteAD)\n\nFor in-place constraints function, use one of the following constructors:\n\nADNLPModel!(f, x0, c!, lcon, ucon)\nADNLPModel!(f, x0, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLPModel!(f, x0, A, c!, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, c!, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, A, c!, lcon, ucon)\n\nwhere the constraint function has the signature c!(output, input).\n\nusing ADNLPModels\nf(x) = sum(x)\nx0 = ones(3)\nfunction c!(output, x) \n  output[1] = 1x[1] + x[2]\n  output[2] = x[2]\nend\nnvar, ncon = 3, 2\nnlp = ADNLPModel!(f, x0, c!, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\n\n\n\n\n\n","category":"type"},{"location":"#ADNLPModels.ADNLSModel","page":"Home","title":"ADNLPModels.ADNLSModel","text":"ADNLSModel(F, x0, nequ)\nADNLSModel(F, x0, nequ, lvar, uvar)\nADNLSModel(F, x0, nequ, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel(F, x0, nequ, A, lcon, ucon)\nADNLSModel(F, x0, nequ, c, lcon, ucon)\nADNLSModel(F, x0, nequ, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLSModel(F, x0, nequ, A, c, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, A, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, c, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, A, c, lcon, ucon)\n\nADNLSModel is an Nonlinear Least Squares model using automatic differentiation to compute the derivatives. The problem is defined as\n\n min  ½‖F(x)‖²\ns.to  lcon ≤ (  Ax  ) ≤ ucon\n             ( c(x) )\n      lvar ≤   x  ≤ uvar\n\nwhere nequ is the size of the vector F(x) and the linear constraints come first.\n\nThe following keyword arguments are available to all constructors:\n\nlinequ: An array of indexes of the linear equations (default: Int[])\nminimize: A boolean indicating whether this is a minimization problem (default: true)\nname: The name of the model (default: \"Generic\")\n\nThe following keyword arguments are available to the constructors for constrained problems:\n\ny0: An inital estimate to the Lagrangian multipliers (default: zeros)\n\nADNLSModel uses ForwardDiff and ReverseDiff for the automatic differentiation. One can specify a new backend with the keyword arguments backend::ADNLPModels.ADBackend. There are three pre-coded backends:\n\nthe default ForwardDiffAD.\nReverseDiffAD.\nZygoteDiffAD accessible after loading Zygote.jl in your environment.\n\nFor an advanced usage, one can define its own backend and redefine the API as done in ADNLPModels.jl/src/forward.jl.\n\nExamples\n\nusing ADNLPModels\nF(x) = [x[2]; x[1]]\nnequ = 2\nx0 = ones(3)\nnvar = 3\nADNLSModel(F, x0, nequ) # uses the default ForwardDiffAD backend.\nADNLSModel(F, x0, nequ; backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nusing Zygote\nADNLSModel(F, x0, nequ; backend = ADNLPModels.ZygoteAD)\n\nusing ADNLPModels\nF(x) = [x[2]; x[1]]\nnequ = 2\nx0 = ones(3)\nc(x) = [1x[1] + x[2]; x[2]]\nnvar, ncon = 3, 2\nADNLSModel(F, x0, nequ, c, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\nADNLSModel(F, x0, nequ, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nusing Zygote\nADNLSModel(F, x0, nequ, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ZygoteAD)\n\nFor in-place constraints and residual function, use one of the following constructors:\n\nADNLSModel!(F!, x0, nequ)\nADNLSModel!(F!, x0, nequ, lvar, uvar)\nADNLSModel!(F!, x0, nequ, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel!(F!, x0, nequ, A, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, A, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, A, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, A, clcon, ucon)\n\nwhere the constraint function has the signature c!(output, input).\n\nusing ADNLPModels\nfunction F!(output, x)\n  output[1] = x[2]\n  output[2] = x[1]\nend\nnequ = 2\nx0 = ones(3)\nfunction c!(output, x) \n  output[1] = 1x[1] + x[2]\n  output[2] = x[2]\nend\nnvar, ncon = 3, 2\nnls = ADNLSModel!(F!, x0, nequ, c!, zeros(ncon), zeros(ncon))\n\n\n\n\n\n","category":"type"},{"location":"","page":"Home","title":"Home","text":"Check the Tutorial for more details on the usage.","category":"page"},{"location":"#License","page":"Home","title":"License","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This content is released under the MPL2.0 License.","category":"page"},{"location":"#Bug-reports-and-discussions","page":"Home","title":"Bug reports and discussions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you think you found a bug, feel free to open an issue. Focused suggestions and requests can also be opened as issues. Before opening a pull request, start an issue or a discussion on the topic, please.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If you want to ask a question not suited for a bug report, feel free to start a discussion here. This forum is for general discussion about this repository and the JuliaSmoothOptimizers, so questions about any of our packages are welcome.","category":"page"},{"location":"#Contents","page":"Home","title":"Contents","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Pages = [\"tutorial.md\"]","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"You can check an Introduction to ADNLPModels.jl on our site, jso.dev.","category":"page"},{"location":"generic/#Creating-an-ADNLPModels-backend-that-supports-multiple-precisions","page":"Support multiple precision","title":"Creating an ADNLPModels backend that supports multiple precisions","text":"","category":"section"},{"location":"generic/","page":"Support multiple precision","title":"Support multiple precision","text":"Pages = [\"generic.md\"]","category":"page"},{"location":"generic/","page":"Support multiple precision","title":"Support multiple precision","text":"You can check the tutorial Creating an ADNLPModels backend that supports multiple precisions on our site, jso.dev.","category":"page"}]
}
