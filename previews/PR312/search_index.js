var documenterSearchIndex = {"docs":
[{"location":"backend/#How-to-switch-backend-in-ADNLPModels","page":"Backend","title":"How to switch backend in ADNLPModels","text":"ADNLPModels allows the use of different backends to compute the derivatives required within NLPModel API. It uses ForwardDiff.jl, ReverseDiff.jl, and more via optional depencies.\n\nThe backend information is in a structure ADNLPModels.ADModelBackend in the attribute adbackend of a ADNLPModel, it can also be accessed with get_adbackend.\n\nThe functions used internally to define the NLPModel API and the possible backends are defined in the following table:\n\nFunctions FowardDiff backends ReverseDiff backends Enzyme backend Sparse backend\ngradient and gradient! ForwardDiffADGradient/GenericForwardDiffADGradient ReverseDiffADGradient/GenericReverseDiffADGradient EnzymeReverseADGradient –\njacobian ForwardDiffADJacobian ReverseDiffADJacobian SparseEnzymeADJacobian SparseADJacobian\nhessian ForwardDiffADHessian ReverseDiffADHessian SparseEnzymeADHessian SparseADHessian/SparseReverseADHessian\nJprod ForwardDiffADJprod/GenericForwardDiffADJprod ReverseDiffADJprod/GenericReverseDiffADJprod EnzymeReverseADJprod –\nJtprod ForwardDiffADJtprod/GenericForwardDiffADJtprod ReverseDiffADJtprod/GenericReverseDiffADJtprod EnzymeReverseADJtprod –\nHvprod ForwardDiffADHvprod/GenericForwardDiffADHvprod ReverseDiffADHvprod/GenericReverseDiffADHvprod EnzymeReverseADHvprod –\ndirectional_second_derivative ForwardDiffADGHjvprod – – –\n\nThe functions hess_structure!, hess_coord!, jac_structure! and jac_coord! defined in ad.jl are generic to all the backends for now.\n\nusing ADNLPModels\nf(x) = sum(x)\nx0 = ones(2)\nADNLPModel(f, x0, show_time = true)\n\nThe keyword show_time is set to true to display the time needed to instantiate each backend. For unconstrained problem, there is no need to compute derivatives of constraints so an EmptyADbackend is used for Jacobian computations.","category":"section"},{"location":"backend/#Examples","page":"Backend","title":"Examples","text":"We now present a serie of practical examples. For simplicity, we focus here on unconstrained optimization problem. All these examples can be generalized to problems with bounds, constraints or nonlinear least-squares.","category":"section"},{"location":"backend/#Use-another-backend","page":"Backend","title":"Use another backend","text":"As shown in Tutorial, it is very straightforward to instantiate an ADNLPModel using an objective function and an initial guess.\n\nusing ADNLPModels, NLPModels\nf(x) = sum(x)\nx0 = ones(3)\nnlp = ADNLPModel(f, x0)\ngrad(nlp, nlp.meta.x0) # returns the gradient at x0\n\nThanks to the backends inside ADNLPModels.jl, it is easy to change the backend for one (or more) function using the kwargs presented in the table above.\n\nnlp = ADNLPModel(f, x0, gradient_backend = ADNLPModels.ReverseDiffADGradient)\ngrad(nlp, nlp.meta.x0)  # returns the gradient at x0 using `ReverseDiff`\n\nIt is also possible to try some new implementation for each function. First, we define a new ADBackend structure.\n\nstruct NewADGradient <: ADNLPModels.ADBackend end\nfunction NewADGradient(\n  nvar::Integer,\n  f,\n  ncon::Integer = 0,\n  c::Function = (args...) -> [];\n  kwargs...,\n)\n  return NewADGradient()\nend\n\nThen, we implement the desired functions following the table above.\n\nADNLPModels.gradient(adbackend::NewADGradient, f, x) = rand(Float64, size(x))\nfunction ADNLPModels.gradient!(adbackend::NewADGradient, g, f, x)\n  g .= rand(Float64, size(x))\n  return g\nend\n\nFinally, we use the homemade backend to compute the gradient.\n\nnlp = ADNLPModel(sum, ones(3), gradient_backend = NewADGradient)\ngrad(nlp, nlp.meta.x0)  # returns the gradient at x0 using `NewADGradient`","category":"section"},{"location":"backend/#Change-backend","page":"Backend","title":"Change backend","text":"Once an instance of an ADNLPModel has been created, it is possible to change the backends without re-instantiating the model.\n\nusing ADNLPModels, NLPModels\nf(x) = 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = 3 * ones(2)\nnlp = ADNLPModel(f, x0)\nget_adbackend(nlp) # returns the `ADModelBackend` structure that regroup all the various backends.\n\nThere are currently two ways to modify instantiated backends. The first one is to instantiate a new ADModelBackend and use set_adbackend! to modify nlp.\n\nadback = ADNLPModels.ADModelBackend(nlp.meta.nvar, nlp.f, gradient_backend = ADNLPModels.ForwardDiffADGradient)\nset_adbackend!(nlp, adback)\nget_adbackend(nlp)\n\nThe alternative is to use set_adbackend! and pass the new backends via kwargs. In the second approach, it is possible to pass either the type of the desired backend or an instance as shown below.\n\nset_adbackend!(\n  nlp,\n  gradient_backend = ADNLPModels.ForwardDiffADGradient,\n  jtprod_backend = ADNLPModels.GenericForwardDiffADJtprod(),\n)\nget_adbackend(nlp)","category":"section"},{"location":"backend/#Support-multiple-precision-without-having-to-recreate-the-model","page":"Backend","title":"Support multiple precision without having to recreate the model","text":"One of the strength of ADNLPModels.jl is the type flexibility. Let's assume, we first instantiate an ADNLPModel with a Float64 initial guess.\n\nusing ADNLPModels, NLPModels\nf(x) = 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = 3 * ones(2) # Float64 initial guess\nnlp = ADNLPModel(f, x0)\n\nThen, the gradient will return a vector of Float64.\n\nx64 = rand(2)\ngrad(nlp, x64)\n\nIt is now possible to move to a different type, for instance Float32, while keeping the instance nlp.\n\nx0_32 = ones(Float32, 2)\nset_adbackend!(nlp, gradient_backend = ADNLPModels.ForwardDiffADGradient, x0 = x0_32)\nx32 = rand(Float32, 2)\ngrad(nlp, x32)","category":"section"},{"location":"performance/#Performance-tips","page":"Performance tips","title":"Performance tips","text":"The package ADNLPModels.jl is designed to easily model optimization problems and to allow an efficient access to the NLPModel API. In this tutorial, we will see some tips to ensure the maximum performance of the model.","category":"section"},{"location":"performance/#Use-in-place-constructor","page":"Performance tips","title":"Use in-place constructor","text":"When dealing with a constrained optimization problem, it is recommended to use in-place constraint functions.\n\nusing ADNLPModels, NLPModels\nf(x) = sum(x)\nx0 = ones(2)\nlcon = ucon = ones(1)\nc_out(x) = [x[1]]\nnlp_out = ADNLPModel(f, x0, c_out, lcon, ucon)\n\nc_in(cx, x) = begin\n  cx[1] = x[1]\n  return cx\nend\nnlp_in = ADNLPModel!(f, x0, c_in, lcon, ucon)\n\nusing BenchmarkTools\ncx = rand(1)\nx = 18 * ones(2)\n@btime cons!(nlp_out, x, cx)\n\n@btime cons!(nlp_in, x, cx)\n\nThe difference between the two increases with the dimension.\n\nNote that the same applies to nonlinear least squares problems.\n\nF(x) = [\n    x[1];\n    x[1] + x[2]^2;\n    sin(x[2]);\n    exp(x[1] + 0.5)\n]\nx0 = ones(2)\nnequ = 4\nnls_out = ADNLSModel(F, x0, nequ)\n\nF!(Fx, x) = begin\n  Fx[1] = x[1]\n  Fx[2] = x[1] + x[2]^2\n  Fx[3] = sin(x[2])\n  Fx[4] = exp(x[1] + 0.5)\n  return Fx\nend\nnls_in = ADNLSModel!(F!, x0, nequ)\n\nFx = rand(4)\n@btime residual!(nls_out, x, Fx)\n\n@btime residual!(nls_in, x, Fx)\n\nThis phenomenon also extends to related backends.\n\nFx = rand(4)\nv = ones(2)\n@btime jprod_residual!(nls_out, x, v, Fx)\n\n@btime jprod_residual!(nls_in, x, v, Fx)","category":"section"},{"location":"performance/#Use-only-the-needed-backends","page":"Performance tips","title":"Use only the needed backends","text":"It is tempting to define the most generic and efficient ADNLPModel from the start.\n\nusing ADNLPModels, NLPModels\nf(x) = (x[1] - x[2])^2\nx0 = ones(2)\nlcon = ucon = ones(1)\nc_in(cx, x) = begin\n  cx[1] = x[1]\n  return cx\nend\nnlp = ADNLPModel!(f, x0, c_in, lcon, ucon, show_time = true)\n\nHowever, depending on the size of the problem this might time consuming as initializing each backend takes time. Besides, some solvers may not require all the API to solve the problem. For instance, Percival.jl is matrix-free solver in the sense that it only uses jprod, jtprod and hprod.\n\nusing Percival\nstats = percival(nlp)\n\nnlp.counters\n\nTherefore, it is more efficient to avoid preparing Jacobian and Hessian backends in this case.\n\nnlp = ADNLPModel!(f, x0, c_in, lcon, ucon, jacobian_backend = ADNLPModels.EmptyADbackend, hessian_backend = ADNLPModels.EmptyADbackend, show_time = true)\n\nor, equivalently, using the matrix_free keyword argument\n\nnlp = ADNLPModel!(f, x0, c_in, lcon, ucon, show_time = true, matrix_free = true)\n\nMore classic nonlinear optimization solvers like Ipopt.jl, KNITRO.jl, or MadNLP.jl only require the gradient and sparse Jacobians and Hessians. This means that we can set all other backends to ADNLPModels.EmptyADbackend.\n\nnlp = ADNLPModel!(f, x0, c_in, lcon, ucon, jprod_backend = ADNLPModels.EmptyADbackend,\n                  jtprod_backend = ADNLPModels.EmptyADbackend, hprod_backend = ADNLPModels.EmptyADbackend,\n                  ghjvprod_backend = ADNLPModels.EmptyADbackend, show_time = true)","category":"section"},{"location":"performance/#Benchmarks","page":"Performance tips","title":"Benchmarks","text":"This package implements several backends for each method and it is possible to design your own backend as well.  Then, one way to choose the most efficient one is to run benchmarks.\n\nusing ADNLPModels, NLPModels, OptimizationProblems\n\nThe package OptimizationProblems.jl provides a collection of optimization problems in JuMP and ADNLPModels syntax.\n\nmeta = OptimizationProblems.meta;\n\nWe select the problems that are scalable, so that there size can be modified. By default, the size is close to 100.\n\nscalable_problems = meta[(meta.variable_nvar .== true) .& (meta.ncon .> 0), :name]\n\nusing NLPModelsJuMP\nlist_backends = Dict(\n  :forward => ADNLPModels.ForwardDiffADGradient,\n  :reverse => ADNLPModels.ReverseDiffADGradient,\n)\n\nusing DataFrames\nnprob = length(scalable_problems)\nstats = Dict{Symbol, DataFrame}()\nfor back in union(keys(list_backends), [:jump])\n  stats[back] = DataFrame(\"name\" => scalable_problems,\n                 \"time\" => zeros(nprob),\n                 \"allocs\" => zeros(Int, nprob))\nend\n\nusing BenchmarkTools\nnscal = 1000\nfor name in scalable_problems\n  n = eval(Meta.parse(\"OptimizationProblems.get_\" * name * \"_nvar(n = $(nscal))\"))\n  m = eval(Meta.parse(\"OptimizationProblems.get_\" * name * \"_ncon(n = $(nscal))\"))\n  @info \" $(name) with $n vars and $m cons\"\n  global x = ones(n)\n  global g = zeros(n)\n  global pb = Meta.parse(name)\n  global nlp = MathOptNLPModel(OptimizationProblems.PureJuMP.eval(pb)(n = nscal))\n  b = @benchmark grad!(nlp, x, g)\n  stats[:jump][stats[:jump].name .== name, :time] = [median(b.times)]\n  stats[:jump][stats[:jump].name .== name, :allocs] = [median(b.allocs)]\n  for back in keys(list_backends)\n    nlp = OptimizationProblems.ADNLPProblems.eval(pb)(n = nscal, gradient_backend = list_backends[back], matrix_free = true)\n    b = @benchmark grad!(nlp, x, g)\n    stats[back][stats[back].name .== name, :time] = [median(b.times)]\n    stats[back][stats[back].name .== name, :allocs] = [median(b.allocs)]\n  end\nend\n\nusing Plots, SolverBenchmark\ncostnames = [\"median time (in ns)\", \"median allocs\"]\ncosts = [\n  df -> df.time,\n  df -> df.allocs,\n]\n\ngr()\n\nprofile_solvers(stats, costs, costnames)","category":"section"},{"location":"predefined/#Default-backend-and-performance-in-ADNLPModels","page":"Default backends","title":"Default backend and performance in ADNLPModels","text":"As illustrated in the tutorial on backends, ADNLPModels.jl use different backend for each method from the NLPModel API that are implemented. By default, it uses the following:\n\nusing ADNLPModels, NLPModels\n\nf(x) = 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nT = Float64\nx0 = T[-1.2; 1.0]\nlvar, uvar = zeros(T, 2), ones(T, 2) # must be of same type than `x0`\nlcon, ucon = -T[0.5], T[0.5]\nc!(cx, x) = begin\n  cx[1] = x[1] + x[2]\n  return cx\nend\nnlp = ADNLPModel!(f, x0, lvar, uvar, c!, lcon, ucon)\nget_adbackend(nlp)\n\nNote that ForwardDiff.jl is mainly used as it is efficient and stable.","category":"section"},{"location":"predefined/#Predefined-backends","page":"Default backends","title":"Predefined backends","text":"Another way to know the default backends used is to check the constant ADNLPModels.default_backend.\n\nADNLPModels.default_backend\n\nMore generally, the package anticipates more uses\n\nADNLPModels.predefined_backend\n\nThe backend :optimized will mainly focus on the most efficient approaches, for instance using ReverseDiff to compute the gradient instead of ForwardDiff.\n\nADNLPModels.predefined_backend[:optimized]\n\nThe backend :generic focuses on backend that make no assumptions on the element type, see Creating an ADNLPModels backend that supports multiple precisions.\n\nIt is possible to use these pre-defined backends using the keyword argument backend when instantiating the model.\n\nnlp = ADNLPModel!(f, x0, lvar, uvar, c!, lcon, ucon, backend = :optimized)\nget_adbackend(nlp)\n\nThe backend :enzyme focuses on backend based on Enzyme.jl.\n\nnlp = ADNLPModel!(f, x0, lvar, uvar, c!, lcon, ucon, backend = :enzyme)\nget_adbackend(nlp)\n\ndanger: Danger\nThe interface for Enzyme.jl is still under development.","category":"section"},{"location":"mixed/#Build-a-hybrid-NLPModel","page":"Build a hybrid NLPModel","title":"Build a hybrid NLPModel","text":"The package ADNLPModels.jl implements the NLPModel API using automatic differentiation (AD) backends. It is also possible to build hybrid models that use AD to complete the implementation of a given NLPModel.\n\nIn the following example, we use ManualNLPModels.jl to build an NLPModel with the gradient and the Jacobian functions implemented.\n\nusing ManualNLPModels\nf(x) = (x[1] - 1)^2 + 4 * (x[2] - x[1]^2)^2\ng!(gx, x) = begin\n  y1, y2 = x[1] - 1, x[2] - x[1]^2\n  gx[1] = 2 * y1 - 16 * x[1] * y2\n  gx[2] = 8 * y2\n  return gx\nend\n\nc!(cx, x) = begin\n  cx[1] = x[1] + x[2]\n  return cx\nend\nj!(vals, x) = begin\n  vals[1] = 1.0\n  vals[2] = 1.0\n  return vals\nend\n\nx0 = [-1.2; 1.0]\nmodel = NLPModel(\n  x0,\n  f,\n  grad = g!,\n  cons = (c!, [0.0], [0.0]),\n  jac_coord = ([1; 1], [1; 2], j!),\n)\n\nHowever, methods involving the Hessian or Jacobian-vector products are not implemented.\n\nusing NLPModels\nv = ones(2)\ntry\n  jprod(model, x0, v)\ncatch e\n  println(\"$e\")\nend\n\nThis is where building hybrid models with ADNLPModels.jl becomes useful.\n\nusing ADNLPModels\nnlp = ADNLPModel!(model, gradient_backend = model, jacobian_backend = model)\n\nThis would be equivalent to do.\n\nnlp = ADNLPModel!(\n  f,\n  x0,\n  c!,\n  [0.0],\n  [0.0],\n  gradient_backend = model,\n  jacobian_backend = model,\n)\n\nget_adbackend(nlp)\n\nNote that the backends used for the gradient and jacobian are now NLPModel. So, a call to grad on nlp\n\ngrad(nlp, x0)\n\nwould call grad on model\n\nneval_grad(model)\n\nMoreover, as expected, the ADNLPModel nlp also implements the missing methods, e.g.\n\njprod(nlp, x0, v)","category":"section"},{"location":"sparsity_pattern/#sparsity-pattern","page":"Providing sparsity pattern for sparse derivatives","title":"Improve sparse derivatives","text":"In this tutorial, we show a feature of ADNLPModels.jl to potentially improve the computation of sparse Jacobian and Hessian.\n\nOur test problem is an academic investment control problem:\n\nbeginaligned\nmin_ux quad  int_0^1 (u(t) - 1) x(t) \n dotx(t) = gamma u(t) x(t)\nendaligned\n\nUsing a simple quadrature formula for the objective functional and a forward finite difference for the differential equation, one can obtain a finite-dimensional continuous optimization problem. One implementation is available in the package OptimizationProblems.jl.\n\nusing ADNLPModels\nusing SparseArrays\n\nT = Float64\nn = 100000\nN = div(n, 2)\nh = 1 // N\nx0 = 1\ngamma = 3\nfunction f(y; N = N, h = h)\n  @views x, u = y[1:N], y[(N + 1):end]\n  return 1 // 2 * h * sum((u[k] - 1) * x[k] + (u[k + 1] - 1) * x[k + 1] for k = 1:(N - 1))\nend\nfunction c!(cx, y; N = N, h = h, gamma = gamma)\n  @views x, u = y[1:N], y[(N + 1):end]\n  for k = 1:(N - 1)\n    cx[k] = x[k + 1] - x[k] - 1 // 2 * h * gamma * (u[k] * x[k] + u[k + 1] * x[k + 1])\n  end\n  return cx\nend\nlvar = vcat(-T(Inf) * ones(T, N), zeros(T, N))\nuvar = vcat(T(Inf) * ones(T, N), ones(T, N))\nxi = vcat(ones(T, N), zeros(T, N))\nlcon = ucon = vcat(one(T), zeros(T, N - 1))\n\n@elapsed begin\n  nlp = ADNLPModel!(f, xi, lvar, uvar, [1], [1], T[1], c!, lcon, ucon; hessian_backend = ADNLPModels.EmptyADbackend)\nend\n\n\nADNLPModel will automatically prepare an AD backend for computing sparse Jacobian and Hessian. We disabled the Hessian computation here to focus the measurement on the Jacobian computation. The keyword argument show_time = true can also be passed to the problem's constructor to get more detailed information about the time used to prepare the AD backend.\n\nusing NLPModels\nx = sqrt(2) * ones(n)\njac_nln(nlp, x)\n\nHowever, it can be rather costly to determine for a given function the sparsity pattern of the Jacobian and the Hessian of the Lagrangian. The good news is that determining this pattern a priori can be relatively straightforward, especially for problems like our optimal control investment problem and other problems with differential equations in the constraints.\n\nThe following example instantiates the Jacobian backend while manually providing the sparsity pattern.\n\nusing ADNLPModels\nusing SparseArrays\n\nT = Float64\nn = 100000\nN = div(n, 2)\nh = 1 // N\nx0 = 1\ngamma = 3\nfunction f(y; N = N, h = h)\n  @views x, u = y[1:N], y[(N + 1):end]\n  return 1 // 2 * h * sum((u[k] - 1) * x[k] + (u[k + 1] - 1) * x[k + 1] for k = 1:(N - 1))\nend\nfunction c!(cx, y; N = N, h = h, gamma = gamma)\n  @views x, u = y[1:N], y[(N + 1):end]\n  for k = 1:(N - 1)\n    cx[k] = x[k + 1] - x[k] - 1 // 2 * h * gamma * (u[k] * x[k] + u[k + 1] * x[k + 1])\n  end\n  return cx\nend\nlvar = vcat(-T(Inf) * ones(T, N), zeros(T, N))\nuvar = vcat(T(Inf) * ones(T, N), ones(T, N))\nxi = vcat(ones(T, N), zeros(T, N))\nlcon = ucon = vcat(one(T), zeros(T, N - 1))\n\n@elapsed begin\n  Is = Vector{Int}(undef, 4 * (N - 1))\n  Js = Vector{Int}(undef, 4 * (N - 1))\n  Vs = ones(Bool, 4 * (N - 1))\n  for i = 1:(N - 1)\n    Is[((i - 1) * 4 + 1):(i * 4)] = [i; i; i; i]\n    Js[((i - 1) * 4 + 1):(i * 4)] = [i; i + 1; N + i; N + i + 1]\n  end\n  J = sparse(Is, Js, Vs, N - 1, n)\n\n  jac_back = ADNLPModels.SparseADJacobian(n, f, N - 1, c!, J)\n  nlp = ADNLPModel!(f, xi, lvar, uvar, [1], [1], T[1], c!, lcon, ucon; hessian_backend = ADNLPModels.EmptyADbackend, jacobian_backend = jac_back)\nend\n\nWe recover the same Jacobian.\n\nusing NLPModels\nx = sqrt(2) * ones(n)\njac_nln(nlp, x)\n\nThe same can be done for the Hessian of the Lagrangian.","category":"section"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/#Contents","page":"Reference","title":"Contents","text":"Pages = [\"reference.md\"]","category":"section"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"Pages = [\"reference.md\"]","category":"section"},{"location":"reference/#ADNLPModels.ADModelBackend","page":"Reference","title":"ADNLPModels.ADModelBackend","text":"ADModelBackend(gradient_backend, hprod_backend, jprod_backend, jtprod_backend, jacobian_backend, hessian_backend, ghjvprod_backend, hprod_residual_backend, jprod_residual_backend, jtprod_residual_backend, jacobian_residual_backend, hessian_residual_backend)\n\nStructure that define the different backend used to compute automatic differentiation of an ADNLPModel/ADNLSModel model. The different backend are all subtype of ADBackend and are respectively used for:\n\ngradient computation;\nhessian-vector products;\njacobian-vector products;\ntranspose jacobian-vector products;\njacobian computation;\nhessian computation;\ndirectional second derivative computation, i.e. gᵀ ∇²cᵢ(x) v.\n\nThe default constructors are      ADModelBackend(nvar, f, ncon = 0, c = (args...) -> []; showtime::Bool = false, kwargs...)     ADModelNLSBackend(nvar, F!, nequ, ncon = 0, c = (args...) -> []; showtime::Bool = false, kwargs...)\n\nIf show_time is set to true, it prints the time used to generate each backend.\n\nThe remaining kwargs are either the different backends as listed below or arguments passed to the backend's constructors:\n\ngradient_backend = ForwardDiffADGradient;\nhprod_backend = ForwardDiffADHvprod;\njprod_backend = ForwardDiffADJprod;\njtprod_backend = ForwardDiffADJtprod;\njacobian_backend = SparseADJacobian;\nhessian_backend = ForwardDiffADHessian;\nghjvprod_backend = ForwardDiffADGHjvprod;\nhprod_residual_backend = ForwardDiffADHvprod for ADNLSModel and EmptyADbackend otherwise;\njprod_residual_backend = ForwardDiffADJprod for ADNLSModel and EmptyADbackend otherwise;\njtprod_residual_backend = ForwardDiffADJtprod for ADNLSModel and EmptyADbackend otherwise;\njacobian_residual_backend = SparseADJacobian for ADNLSModel and EmptyADbackend otherwise;\nhessian_residual_backend = ForwardDiffADHessian for ADNLSModel and EmptyADbackend otherwise.\n\n\n\n\n\n","category":"type"},{"location":"reference/#ADNLPModels.ADNLPModel-Union{Tuple{S}, Tuple{Any, S}} where S","page":"Reference","title":"ADNLPModels.ADNLPModel","text":"ADNLPModel(f, x0)\nADNLPModel(f, x0, lvar, uvar)\nADNLPModel(f, x0, clinrows, clincols, clinvals, lcon, ucon)\nADNLPModel(f, x0, A, lcon, ucon)\nADNLPModel(f, x0, c, lcon, ucon)\nADNLPModel(f, x0, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLPModel(f, x0, A, c, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, A, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, c, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, A, c, lcon, ucon)\nADNLPModel(model::AbstractNLPModel)\n\nADNLPModel is an AbstractNLPModel using automatic differentiation to compute the derivatives. The problem is defined as\n\n min  f(x)\ns.to  lcon ≤ (  Ax  ) ≤ ucon\n             ( c(x) )\n      lvar ≤   x  ≤ uvar.\n\nThe following keyword arguments are available to all constructors:\n\nminimize: A boolean indicating whether this is a minimization problem (default: true)\nname: The name of the model (default: \"Generic\")\n\nThe following keyword arguments are available to the constructors for constrained problems:\n\ny0: An inital estimate to the Lagrangian multipliers (default: zeros)\n\nADNLPModel uses ForwardDiff and ReverseDiff for the automatic differentiation. One can specify a new backend with the keyword arguments backend::ADNLPModels.ADBackend. There are three pre-coded backends:\n\nthe default ForwardDiffAD.\nReverseDiffAD.\n\nFor an advanced usage, one can define its own backend and redefine the API as done in ADNLPModels.jl/src/forward.jl.\n\nExamples\n\nusing ADNLPModels\nf(x) = sum(x)\nx0 = ones(3)\nnvar = 3\nADNLPModel(f, x0) # uses the default ForwardDiffAD backend.\nADNLPModel(f, x0; backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nusing ADNLPModels\nf(x) = sum(x)\nx0 = ones(3)\nc(x) = [1x[1] + x[2]; x[2]]\nnvar, ncon = 3, 2\nADNLPModel(f, x0, c, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\nADNLPModel(f, x0, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nFor in-place constraints function, use one of the following constructors:\n\nADNLPModel!(f, x0, c!, lcon, ucon)\nADNLPModel!(f, x0, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLPModel!(f, x0, A, c!, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, c!, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, A, c!, lcon, ucon)\nADNLSModel!(model::AbstractNLSModel)\n\nwhere the constraint function has the signature c!(output, input).\n\nusing ADNLPModels\nf(x) = sum(x)\nx0 = ones(3)\nfunction c!(output, x) \n  output[1] = 1x[1] + x[2]\n  output[2] = x[2]\nend\nnvar, ncon = 3, 2\nnlp = ADNLPModel!(f, x0, c!, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.ADNLSModel-Union{Tuple{S}, Tuple{Any, S, Integer}} where S","page":"Reference","title":"ADNLPModels.ADNLSModel","text":"ADNLSModel(F, x0, nequ)\nADNLSModel(F, x0, nequ, lvar, uvar)\nADNLSModel(F, x0, nequ, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel(F, x0, nequ, A, lcon, ucon)\nADNLSModel(F, x0, nequ, c, lcon, ucon)\nADNLSModel(F, x0, nequ, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLSModel(F, x0, nequ, A, c, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, A, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, c, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, A, c, lcon, ucon)\nADNLSModel(model::AbstractNLSModel)\n\nADNLSModel is an Nonlinear Least Squares model using automatic differentiation to compute the derivatives. The problem is defined as\n\n min  ½‖F(x)‖²\ns.to  lcon ≤ (  Ax  ) ≤ ucon\n             ( c(x) )\n      lvar ≤   x  ≤ uvar\n\nwhere nequ is the size of the vector F(x) and the linear constraints come first.\n\nThe following keyword arguments are available to all constructors:\n\nlinequ: An array of indexes of the linear equations (default: Int[])\nminimize: A boolean indicating whether this is a minimization problem (default: true)\nname: The name of the model (default: \"Generic\")\n\nThe following keyword arguments are available to the constructors for constrained problems:\n\ny0: An inital estimate to the Lagrangian multipliers (default: zeros)\n\nADNLSModel uses ForwardDiff and ReverseDiff for the automatic differentiation. One can specify a new backend with the keyword arguments backend::ADNLPModels.ADBackend. There are three pre-coded backends:\n\nthe default ForwardDiffAD.\nReverseDiffAD.\n\nFor an advanced usage, one can define its own backend and redefine the API as done in ADNLPModels.jl/src/forward.jl.\n\nExamples\n\nusing ADNLPModels\nF(x) = [x[2]; x[1]]\nnequ = 2\nx0 = ones(3)\nnvar = 3\nADNLSModel(F, x0, nequ) # uses the default ForwardDiffAD backend.\nADNLSModel(F, x0, nequ; backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nusing ADNLPModels\nF(x) = [x[2]; x[1]]\nnequ = 2\nx0 = ones(3)\nc(x) = [1x[1] + x[2]; x[2]]\nnvar, ncon = 3, 2\nADNLSModel(F, x0, nequ, c, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\nADNLSModel(F, x0, nequ, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nFor in-place constraints and residual function, use one of the following constructors:\n\nADNLSModel!(F!, x0, nequ)\nADNLSModel!(F!, x0, nequ, lvar, uvar)\nADNLSModel!(F!, x0, nequ, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel!(F!, x0, nequ, A, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, A, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, A, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, A, clcon, ucon)\nADNLSModel!(model::AbstractNLSModel)\n\nwhere the constraint function has the signature c!(output, input).\n\nusing ADNLPModels\nfunction F!(output, x)\n  output[1] = x[2]\n  output[2] = x[1]\nend\nnequ = 2\nx0 = ones(3)\nfunction c!(output, x) \n  output[1] = 1x[1] + x[2]\n  output[2] = x[2]\nend\nnvar, ncon = 3, 2\nnls = ADNLSModel!(F!, x0, nequ, c!, zeros(ncon), zeros(ncon))\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.compute_hessian_sparsity-NTuple{4, Any}","page":"Reference","title":"ADNLPModels.compute_hessian_sparsity","text":"compute_hessian_sparsity(f, nvar, c!, ncon; detector)\n\nReturn a sparse boolean matrix that represents the adjacency matrix of the Hessian of f(x) + λᵀc(x).\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.compute_jacobian_sparsity","page":"Reference","title":"ADNLPModels.compute_jacobian_sparsity","text":"compute_jacobian_sparsity(c, x0; detector)\ncompute_jacobian_sparsity(c!, cx, x0; detector)\n\nReturn a sparse boolean matrix that represents the adjacency matrix of the Jacobian of c(x).\n\n\n\n\n\n","category":"function"},{"location":"reference/#ADNLPModels.get_F-Tuple{ADNLPModels.AbstractADNLSModel}","page":"Reference","title":"ADNLPModels.get_F","text":"get_F(nls)\nget_F(nls, ::ADBackend)\n\nReturn the out-of-place version of nls.F!.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_adbackend-Tuple{Union{ADNLPModels.AbstractADNLPModel{T, S}, ADNLPModels.AbstractADNLSModel{T, S}} where {T, S}}","page":"Reference","title":"ADNLPModels.get_adbackend","text":"get_adbackend(nlp)\n\nReturns the value adbackend from nlp.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_c-Tuple{Union{ADNLPModels.AbstractADNLPModel{T, S}, ADNLPModels.AbstractADNLSModel{T, S}} where {T, S}}","page":"Reference","title":"ADNLPModels.get_c","text":"get_c(nlp)\nget_c(nlp, ::ADBackend)\n\nReturn the out-of-place version of nlp.c!.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_default_backend-Tuple{Symbol, Vararg{Any}}","page":"Reference","title":"ADNLPModels.get_default_backend","text":"get_default_backend(meth::Symbol, backend::Symbol; kwargs...)\nget_default_backend(::Val{::Symbol}, backend; kwargs...)\n\nReturn a type <:ADBackend that corresponds to the default backend use for the method meth. See keys(ADNLPModels.predefined_backend) for a list of possible backends.\n\nThe following keyword arguments are accepted:\n\nmatrix_free::Bool: If true, this returns an EmptyADbackend for methods that handle matrices, e.g. :hessian_backend.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_lag-Tuple{ADNLPModels.AbstractADNLPModel, ADNLPModels.ADBackend, Real}","page":"Reference","title":"ADNLPModels.get_lag","text":"get_lag(nlp, b::ADBackend, obj_weight)\nget_lag(nlp, b::ADBackend, obj_weight, y)\n\nReturn the lagrangian function ℓ(x) = obj_weight * f(x) + c(x)ᵀy.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_nln_nnzh-Tuple{ADNLPModels.ADModelBackend, Any}","page":"Reference","title":"ADNLPModels.get_nln_nnzh","text":"get_nln_nnzh(::ADBackend, nvar)\nget_nln_nnzh(b::ADModelBackend, nvar)\nget_nln_nnzh(nlp::AbstractNLPModel, nvar)\n\nFor a given ADBackend of a problem with nvar variables, return the number of nonzeros in the lower triangle of the Hessian. If b is the ADModelBackend then b.hessian_backend is used.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_nln_nnzj-Tuple{ADNLPModels.ADModelBackend, Any, Any}","page":"Reference","title":"ADNLPModels.get_nln_nnzj","text":"get_nln_nnzj(::ADBackend, nvar, ncon)\nget_nln_nnzj(b::ADModelBackend, nvar, ncon)\nget_nln_nnzj(nlp::AbstractNLPModel, nvar, ncon)\n\nFor a given ADBackend of a problem with nvar variables and ncon constraints, return the number of nonzeros in the Jacobian of nonlinear constraints. If b is the ADModelBackend then b.jacobian_backend is used.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_residual_nnzh-Tuple{ADNLPModels.ADModelBackend, Any}","page":"Reference","title":"ADNLPModels.get_residual_nnzh","text":"get_residual_nnzh(b::ADModelBackend, nvar)\nget_residual_nnzh(nls::AbstractNLSModel, nvar)\n\nReturn the number of nonzeros elements in the residual Hessians.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_residual_nnzj-Tuple{ADNLPModels.ADModelBackend, Any, Any}","page":"Reference","title":"ADNLPModels.get_residual_nnzj","text":"get_residual_nnzj(b::ADModelBackend, nvar, nequ)\nget_residual_nnzj(nls::AbstractNLSModel, nvar, nequ)\n\nReturn the number of nonzeros elements in the residual Jacobians.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.get_sparsity_pattern-Tuple{Union{ADNLPModels.AbstractADNLPModel{T, S}, ADNLPModels.AbstractADNLSModel{T, S}} where {T, S}, Symbol}","page":"Reference","title":"ADNLPModels.get_sparsity_pattern","text":"S = get_sparsity_pattern(model::ADModel, derivative::Symbol)\n\nRetrieve the sparsity pattern of a Jacobian or Hessian from an ADModel. For the Hessian, only the lower triangular part of its sparsity pattern is returned. The user can reconstruct the upper triangular part by exploiting symmetry.\n\nTo compute the sparsity pattern, the model must use a sparse backend. Supported backends include SparseADJacobian, SparseADHessian, and SparseReverseADHessian.\n\nInput arguments\n\nmodel: An automatic differentiation model (either AbstractADNLPModel or AbstractADNLSModel).\nderivative: The type of derivative for which the sparsity pattern is needed. The supported values are :jacobian, :hessian, :jacobian_residual and :hessian_residual.\n\nOutput argument\n\nS: A sparse matrix of type SparseMatrixCSC{Bool,Int} indicating the sparsity pattern of the requested derivative.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ADNLPModels.set_adbackend!-Tuple{Union{ADNLPModels.AbstractADNLPModel{T, S}, ADNLPModels.AbstractADNLSModel{T, S}} where {T, S}, ADNLPModels.ADModelBackend}","page":"Reference","title":"ADNLPModels.set_adbackend!","text":"set_adbackend!(nlp, new_adbackend)\nset_adbackend!(nlp; kwargs...)\n\nReplace the current adbackend value of nlp by new_adbackend or instantiate a new one with kwargs, see ADModelBackend. By default, the setter with kwargs will reuse existing backends.\n\n\n\n\n\n","category":"method"},{"location":"sparse/#sparse","page":"Sparse Jacobian and Hessian","title":"Sparse Hessian and Jacobian computations","text":"By default, the Jacobian and Hessian are treated as sparse.\n\nusing ADNLPModels, NLPModels\n\nf(x) = (x[1] - 1)^2\nT = Float64\nx0 = T[-1.2; 1.0]\nnvar, ncon = 2, 1\nlvar, uvar = zeros(T, nvar), ones(T, nvar)\nlcon, ucon = -T[0.5], T[0.5]\nc!(cx, x) = begin\n  cx[1] = x[2]\n  return cx\nend\nnlp = ADNLPModel!(f, x0, lvar, uvar, c!, lcon, ucon, backend = :optimized)\n\n(get_nnzj(nlp), get_nnzh(nlp))  # Number of nonzero elements in the Jacobian and Hessian\n\nx = rand(T, nvar)\nJ = jac(nlp, x)\n\nx = rand(T, nvar)\nH = hess(nlp, x)","category":"section"},{"location":"sparse/#Options-for-sparsity-pattern-detection-and-coloring","page":"Sparse Jacobian and Hessian","title":"Options for sparsity pattern detection and coloring","text":"The backends available for sparse derivatives (SparseADJacobian, SparseEnzymeADJacobian, SparseADHessian, SparseReverseADHessian, and SparseEnzymeADHessian) allow for customization through keyword arguments such as detector and coloring_algorithm. These arguments specify the sparsity pattern detector and the coloring algorithm, respectively.\n\nA detector must be of type ADTypes.AbstractSparsityDetector. The default detector is TracerSparsityDetector() from the package SparseConnectivityTracer.jl. Prior to version 0.8.0, the default was SymbolicSparsityDetector() from Symbolics.jl. A TracerLocalSparsityDetector() is also available and can be used if the sparsity pattern of Jacobians and Hessians depends on x.\n\nimport SparseConnectivityTracer.TracerLocalSparsityDetector\n\nset_adbackend!(\n  nlp,\n  jacobian_backend = ADNLPModels.SparseADJacobian(nvar, f, ncon, c!, detector=TracerLocalSparsityDetector()),\n  hessian_backend = ADNLPModels.SparseADHessian(nvar, f, ncon, c!, detector=TracerLocalSparsityDetector()),\n)\n\nA coloring_algorithm must be of type SparseMatrixColorings.GreedyColoringAlgorithm. The default algorithm is GreedyColoringAlgorithm{:direct}() for SparseADJacobian, SparseEnzymeADJacobian and SparseADHessian, while it is GreedyColoringAlgorithm{:substitution}() for SparseReverseADHessian and SparseEnzymeADHessian. These algorithms are provided by the package SparseMatrixColorings.jl.\n\nusing SparseMatrixColorings\n\nset_adbackend!(\n  nlp,\n  hessian_backend = ADNLPModels.SparseADHessian(nvar, f, ncon, c!, coloring_algorithm=GreedyColoringAlgorithm{:substitution}()),\n)\n\nThe GreedyColoringAlgorithm{:direct}() performs column coloring for Jacobians and star coloring for Hessians. In contrast, GreedyColoringAlgorithm{:substitution}() applies acyclic coloring for Hessians. The :substitution mode generally requires fewer colors than :direct, thus fewer directional derivatives are needed to reconstruct the sparse Hessian. However, it necessitates storing the compressed sparse Hessian, while :direct coloring only requires storage for one column of the compressed Hessian.\n\nThe :direct coloring mode is numerically more stable and may be preferable for highly ill-conditioned Hessians, as it avoids solving triangular systems to compute nonzero entries from the compressed Hessian.","category":"section"},{"location":"sparse/#Extracting-sparsity-patterns","page":"Sparse Jacobian and Hessian","title":"Extracting sparsity patterns","text":"ADNLPModels.jl provides the function get_sparsity_pattern to retrieve the sparsity patterns of the Jacobian or Hessian from a model.\n\nusing SparseArrays, ADNLPModels, NLPModels\n\nnvar = 10\nncon = 5\n\nf(x) = sum((x[i] - i)^2 for i = 1:nvar) + x[nvar] * sum(x[j] for j = 1:nvar-1)\n\nfunction c!(cx, x)\n  cx[1] = x[1] + x[2]\n  cx[2] = x[1] + x[2] + x[3]\n  cx[3] = x[2] + x[3] + x[4]\n  cx[4] = x[3] + x[4] + x[5]\n  cx[5] = x[4] + x[5]\n  return cx\nend\n\nT = Float64\nx0 = -ones(T, nvar)\nlvar = zeros(T, nvar)\nuvar = 2 * ones(T, nvar)\nlcon = -0.5 * ones(T, ncon)\nucon = 0.5 * ones(T, ncon)\n\nnlp = ADNLPModel!(f, x0, lvar, uvar, c!, lcon, ucon)\n\nJ = get_sparsity_pattern(nlp, :jacobian)\n\nH = get_sparsity_pattern(nlp, :hessian)","category":"section"},{"location":"sparse/#Using-known-sparsity-patterns","page":"Sparse Jacobian and Hessian","title":"Using known sparsity patterns","text":"If the sparsity pattern of the Jacobian or the Hessian is already known, you can provide it directly. This may happen when the pattern is derived from the application or has been computed previously and saved for reuse. Note that both the lower and upper triangular parts of the Hessian are required during the coloring phase.\n\nusing SparseArrays, ADNLPModels, NLPModels\n\nnvar = 10\nncon = 5\n\nf(x) = sum((x[i] - i)^2 for i = 1:nvar) + x[nvar] * sum(x[j] for j = 1:nvar-1)\n\nH = SparseMatrixCSC{Bool, Int}(\n  [ 1  0  0  0  0  0  0  0  0  1 ;\n    0  1  0  0  0  0  0  0  0  1 ;\n    0  0  1  0  0  0  0  0  0  1 ;\n    0  0  0  1  0  0  0  0  0  1 ;\n    0  0  0  0  1  0  0  0  0  1 ;\n    0  0  0  0  0  1  0  0  0  1 ;\n    0  0  0  0  0  0  1  0  0  1 ;\n    0  0  0  0  0  0  0  1  0  1 ;\n    0  0  0  0  0  0  0  0  1  1 ;\n    1  1  1  1  1  1  1  1  1  1 ]\n)\n\nfunction c!(cx, x)\n  cx[1] = x[1] + x[2]\n  cx[2] = x[1] + x[2] + x[3]\n  cx[3] = x[2] + x[3] + x[4]\n  cx[4] = x[3] + x[4] + x[5]\n  cx[5] = x[4] + x[5]\n  return cx\nend\n\nJ = SparseMatrixCSC{Bool, Int}(\n  [ 1  1  0  0  0  0  0  0  0  0 ;\n    1  1  1  0  0  0  0  0  0  0 ;\n    0  1  1  1  0  0  0  0  0  0 ;\n    0  0  1  1  1  0  0  0  0  0 ;\n    0  0  0  1  1  0  0  0  0  0 ]\n)\n\nT = Float64\nx0 = -ones(T, nvar)\nlvar = zeros(T, nvar)\nuvar = 2 * ones(T, nvar)\nlcon = -0.5 * ones(T, ncon)\nucon = 0.5 * ones(T, ncon)\n\nJ_backend = ADNLPModels.SparseADJacobian(nvar, f, ncon, c!, J)\nH_backend = ADNLPModels.SparseADHessian(nvar, f, ncon, c!, H)\n\nnlp = ADNLPModel!(f, x0, lvar, uvar, c!, lcon, ucon, jacobian_backend=J_backend, hessian_backend=H_backend)\n\nThe section \"providing the sparsity pattern for sparse derivatives\" illustrates this feature with a more advanced application.","category":"section"},{"location":"sparse/#Automatic-sparse-differentiation-(ASD)","page":"Sparse Jacobian and Hessian","title":"Automatic sparse differentiation (ASD)","text":"For a deeper understanding of how ADNLPModels.jl computes sparse Jacobians and Hessians, you can refer to the following blog post: \"An Illustrated Guide to Automatic Sparse Differentiation\". It explains the key ideas behind sparse automatic differentiation (ASD), and why this approach is critical for large-scale nonlinear optimization.","category":"section"},{"location":"sparse/#Acknowledgements","page":"Sparse Jacobian and Hessian","title":"Acknowledgements","text":"The package SparseConnectivityTracer.jl is used to compute the sparsity pattern of Jacobians and Hessians. The evaluation of the number of directional derivatives and the seeds required to compute compressed Jacobians and Hessians is performed using SparseMatrixColorings.jl. As of release v0.8.1, it has replaced ColPack.jl. We acknowledge Guillaume Dalle (@gdalle), Adrian Hill (@adrhill), Alexis Montoison (@amontoison), and Michel Schanen (@michel2323) for the development of these packages.","category":"section"},{"location":"#ADNLPModels","page":"Home","title":"ADNLPModels","text":"This package provides automatic differentiation (AD)-based model implementations that conform to the NLPModels API. The general form of the optimization problem is\n\nbeginaligned\nmin quad  f(x) \n c_L leq c(x) leq c_U \n ell leq x leq u\nendaligned","category":"section"},{"location":"#Install","page":"Home","title":"Install","text":"ADNLPModels Julia Language package. To install ADNLPModels, please open Julia's interactive session (known as REPL) and press the ] key in the REPL to use the package mode, then type the following command\n\npkg> add ADNLPModels","category":"section"},{"location":"#Complementary-packages","page":"Home","title":"Complementary packages","text":"ADNLPModels.jl functionalities are extended by other packages that are not automatically loaded. In other words, you sometimes need to load the desired package separately to access some functionalities.\n\nusing ADNLPModels  # load only the default functionalities\nusing Enzyme       # load the Enzyme backends\n\nVersions compatibility for the extensions are available in the file test/Project.toml.\n\nprint(open(io->read(io, String), \"../../test/Project.toml\"))","category":"section"},{"location":"#Usage","page":"Home","title":"Usage","text":"This package defines two models, ADNLPModel for general nonlinear optimization, and ADNLSModel for nonlinear least-squares problems.\n\nCheck the Tutorial for more details on the usage.","category":"section"},{"location":"#License","page":"Home","title":"License","text":"This content is released under the MPL2.0 License.","category":"section"},{"location":"#Bug-reports-and-discussions","page":"Home","title":"Bug reports and discussions","text":"If you think you found a bug, feel free to open an issue. Focused suggestions and requests can also be opened as issues. Before opening a pull request, start an issue or a discussion on the topic, please.\n\nIf you want to ask a question not suited for a bug report, feel free to start a discussion here. This forum is for general discussion about this repository and the JuliaSmoothOptimizers, so questions about any of our packages are welcome.","category":"section"},{"location":"#Contents","page":"Home","title":"Contents","text":"","category":"section"},{"location":"#ADNLPModels.ADNLPModel","page":"Home","title":"ADNLPModels.ADNLPModel","text":"ADNLPModel(f, x0)\nADNLPModel(f, x0, lvar, uvar)\nADNLPModel(f, x0, clinrows, clincols, clinvals, lcon, ucon)\nADNLPModel(f, x0, A, lcon, ucon)\nADNLPModel(f, x0, c, lcon, ucon)\nADNLPModel(f, x0, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLPModel(f, x0, A, c, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, A, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, c, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, A, c, lcon, ucon)\nADNLPModel(model::AbstractNLPModel)\n\nADNLPModel is an AbstractNLPModel using automatic differentiation to compute the derivatives. The problem is defined as\n\n min  f(x)\ns.to  lcon ≤ (  Ax  ) ≤ ucon\n             ( c(x) )\n      lvar ≤   x  ≤ uvar.\n\nThe following keyword arguments are available to all constructors:\n\nminimize: A boolean indicating whether this is a minimization problem (default: true)\nname: The name of the model (default: \"Generic\")\n\nThe following keyword arguments are available to the constructors for constrained problems:\n\ny0: An inital estimate to the Lagrangian multipliers (default: zeros)\n\nADNLPModel uses ForwardDiff and ReverseDiff for the automatic differentiation. One can specify a new backend with the keyword arguments backend::ADNLPModels.ADBackend. There are three pre-coded backends:\n\nthe default ForwardDiffAD.\nReverseDiffAD.\n\nFor an advanced usage, one can define its own backend and redefine the API as done in ADNLPModels.jl/src/forward.jl.\n\nExamples\n\nusing ADNLPModels\nf(x) = sum(x)\nx0 = ones(3)\nnvar = 3\nADNLPModel(f, x0) # uses the default ForwardDiffAD backend.\nADNLPModel(f, x0; backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nusing ADNLPModels\nf(x) = sum(x)\nx0 = ones(3)\nc(x) = [1x[1] + x[2]; x[2]]\nnvar, ncon = 3, 2\nADNLPModel(f, x0, c, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\nADNLPModel(f, x0, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nFor in-place constraints function, use one of the following constructors:\n\nADNLPModel!(f, x0, c!, lcon, ucon)\nADNLPModel!(f, x0, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLPModel!(f, x0, A, c!, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, c!, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, A, c!, lcon, ucon)\nADNLSModel!(model::AbstractNLSModel)\n\nwhere the constraint function has the signature c!(output, input).\n\nusing ADNLPModels\nf(x) = sum(x)\nx0 = ones(3)\nfunction c!(output, x) \n  output[1] = 1x[1] + x[2]\n  output[2] = x[2]\nend\nnvar, ncon = 3, 2\nnlp = ADNLPModel!(f, x0, c!, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\n\n\n\n\n\n","category":"type"},{"location":"#ADNLPModels.ADNLSModel","page":"Home","title":"ADNLPModels.ADNLSModel","text":"ADNLSModel(F, x0, nequ)\nADNLSModel(F, x0, nequ, lvar, uvar)\nADNLSModel(F, x0, nequ, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel(F, x0, nequ, A, lcon, ucon)\nADNLSModel(F, x0, nequ, c, lcon, ucon)\nADNLSModel(F, x0, nequ, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLSModel(F, x0, nequ, A, c, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, A, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, c, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, c, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, A, c, lcon, ucon)\nADNLSModel(model::AbstractNLSModel)\n\nADNLSModel is an Nonlinear Least Squares model using automatic differentiation to compute the derivatives. The problem is defined as\n\n min  ½‖F(x)‖²\ns.to  lcon ≤ (  Ax  ) ≤ ucon\n             ( c(x) )\n      lvar ≤   x  ≤ uvar\n\nwhere nequ is the size of the vector F(x) and the linear constraints come first.\n\nThe following keyword arguments are available to all constructors:\n\nlinequ: An array of indexes of the linear equations (default: Int[])\nminimize: A boolean indicating whether this is a minimization problem (default: true)\nname: The name of the model (default: \"Generic\")\n\nThe following keyword arguments are available to the constructors for constrained problems:\n\ny0: An inital estimate to the Lagrangian multipliers (default: zeros)\n\nADNLSModel uses ForwardDiff and ReverseDiff for the automatic differentiation. One can specify a new backend with the keyword arguments backend::ADNLPModels.ADBackend. There are three pre-coded backends:\n\nthe default ForwardDiffAD.\nReverseDiffAD.\n\nFor an advanced usage, one can define its own backend and redefine the API as done in ADNLPModels.jl/src/forward.jl.\n\nExamples\n\nusing ADNLPModels\nF(x) = [x[2]; x[1]]\nnequ = 2\nx0 = ones(3)\nnvar = 3\nADNLSModel(F, x0, nequ) # uses the default ForwardDiffAD backend.\nADNLSModel(F, x0, nequ; backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nusing ADNLPModels\nF(x) = [x[2]; x[1]]\nnequ = 2\nx0 = ones(3)\nc(x) = [1x[1] + x[2]; x[2]]\nnvar, ncon = 3, 2\nADNLSModel(F, x0, nequ, c, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\nADNLSModel(F, x0, nequ, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n\nFor in-place constraints and residual function, use one of the following constructors:\n\nADNLSModel!(F!, x0, nequ)\nADNLSModel!(F!, x0, nequ, lvar, uvar)\nADNLSModel!(F!, x0, nequ, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel!(F!, x0, nequ, A, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, A, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, clinrows, clincols, clinvals, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, A, c!, lcon, ucon)\nADNLSModel!(F!, x0, nequ, lvar, uvar, A, clcon, ucon)\nADNLSModel!(model::AbstractNLSModel)\n\nwhere the constraint function has the signature c!(output, input).\n\nusing ADNLPModels\nfunction F!(output, x)\n  output[1] = x[2]\n  output[2] = x[1]\nend\nnequ = 2\nx0 = ones(3)\nfunction c!(output, x) \n  output[1] = 1x[1] + x[2]\n  output[2] = x[2]\nend\nnvar, ncon = 3, 2\nnls = ADNLSModel!(F!, x0, nequ, c!, zeros(ncon), zeros(ncon))\n\n\n\n\n\n","category":"type"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"Pages = [\"tutorial.md\"]\n\nYou can check an Introduction to ADNLPModels.jl on our site, jso.dev.","category":"section"},{"location":"generic/#Creating-an-ADNLPModels-backend-that-supports-multiple-precisions","page":"Support multiple precision","title":"Creating an ADNLPModels backend that supports multiple precisions","text":"Pages = [\"generic.md\"]\n\nYou can check the tutorial Creating an ADNLPModels backend that supports multiple precisions on our site, jso.dev.","category":"section"}]
}
